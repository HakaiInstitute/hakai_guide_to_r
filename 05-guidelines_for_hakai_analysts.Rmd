# Guidelines for Developing Analyses with the Hakai Institute

This chapter is currently under development.

Worth talking about here, the difference methods to collaborate -> fork and pull
public repos or direct editing capability of the repo
## Step-by-step Analysis Example

In this section I will walk you through the steps that brings together all the different concepts we've been talking about up to this point. This step by step guide should serve as an example of how to develop analyses when working with the Hakai Institute.

### Set up a repo on GitHUB

If you are working on a specific program, there's a good chance your program already has a central GitHUB repo set up for all your programs analyses. If you've been asked to conduct some data summaries or analyses you should talk to your PI about storing the code you write in your program's repo.
If your program doesn't have a repo, you're going to want to set one up. Make sure you talk to your PI and someone from Hakai IT about  this.

Whether you're setting up a repo on your own GitHub account or the Hakai one, your going to want to first create the repo, and then create your R-Studio project by following these instructions. If the repo you want to use already exists, then just go straight to the next section: 'Start a new R-Studio project.'

1) Go to https://github.com and login.

2) Create a new repository on your own account, or on the Hakai account depending on the scenario.

3) Call it something-descriptive-but-concise. 

4) Make it private to start (talk to your PI or IT about making it Public) and click yes to initialize with a README.

5) Click the big green button “Create repository.”

6) Copy the HTTPS clone URL to your clipboard via the green “Clone or Download” button. You'll need this to link the repo with R-Studio.

### Start a new R-Studio project

If you have just created a general repo for your program, you should create a directory on your local drive called Analyses. This folder will be put under version control and you can decide which scripts are submitted to the repo and made public.

To link the gitHUB repo to your google drive folder and a new project in R-Studio:

1) Open R-Studio and go to File > New Project > Version Control > Git. In the “repository URL” paste the URL of the  Git Hub repository you just made. Under project directory name, type in the name of whatever this analysis is, for example "soft sediment spp diversity" — this will be the name of the analysis' directory. Under 'Create project as a sub-directory of' navigate to your programs Google Drive folder for analyses. Click “Create Project”.

2) Go to Tools in the menu bar, and navigate to Project Options. Click Packrat, and then check the box 'Use Packrat with this project'. This may take a few minutes to initialize, but is important to make your analyses reproducibile in the long term.

This should download the README.md file that we created on GitHub in the previous step. Look in RStudio’s file browser pane for the README.md file."

### Create folder structure

I use a default folder structure for every analysis based on the files that are produced from every analysis. Using the project directory that you created your new R-Studio project, create these sub-folders within the project folder:

* raw data
* processed data 
* scripts
* figures


## Analysis Workflow

Here is a general workflow I typically adhere to, and could be adopted as a starting point from which individual analysts could modify.

### Import, pre-process & tidy data

I usually create a few different scripts in any analysis, which helps me to compartmentalize the different steps of the analysis. I start with a pre-processing script that will read in and format all the different data sets I want to combine and analyze. This is where I make changes to data format that I want to apply to all subsequent uses of that data (so that I don't have to keep doing it in subsequent scripts).


1) __Create pre_process.R script__ In your newly created R Studio project, go to File > New File > R Script. Save it in the raw scripts sub-directory of your project directory.

2) Set your working directory

3) Load packages

4) Import data:

__Method 1__:From spreadsheet you already have  

Most often you're going to want to read in files that are .csv files. These are comma separated value files and can be produced from excel or Google Sheets by saving your excel or Google Sheet file as a .csv file.

The first module of an analysis I produce is a plain .R script that loads in my .csv data file and save it in my R environment as a tibble, a tidy table, using the `new_tbl <- read_csv("filepath")` format. Before you read in a file, you should load the packages that we will be required for every analysis you conduct using the `library(tidyverse)` function. Note that you should not use the base R function `read.csv` but rather use the tidy-verse function `read_csv`. The base version will inevitably cause frustration due to incorrect variable class assignment.

__Method 2__: From Google Drive

__Method 3__:From Hakai database

It is possible to get data from the Hakai database directly into R without having to leave R Studio. This is accomplished by interacting with an application programming interface (API) was developed for downloading data from Hakai's data portal. Earlier in the Data Analysts Toolbox section we installed Hakai's API. Now we will use it to get Hakai data from the database.

Below is a quickstart example of how you can download some chlorophyll data. To be able to access Hakai data you must sign in with a Hakai email address that has the appropriate permissions to access the database. This authentication is done using a token. When you run the code below, a web URL will be displayed in the console. Copy and paste that URL into your browser. This should take to you a webpage that displays another web URL, this is your authentication token. Copy and paste the URL into the console in R Studio where it tells you to do so.

```
library('hakaiApi')

# Get the api request client, run this line independently before the rest of the code
client <- hakaiApi::Client$new() # Follow stdout prompts to get an API token

# Make a data request for chlorophyll data
endpoint <- sprintf("%s/%s", client$api_root, "eims/views/output/chlorophyll?limit=50")
data <- client$get(endpoint)

# Print out the data
print(data)
```
By running this code you should see chlorophyll data in your environment. 
The above code can be modified to select different datasets other than chlorophyll and filter based on different logical parameters you set. This is accomplished by editing the text after the ? in `"eims/views/output/chlorophyll?limit=50"`.

The formula you set after the question mark is known as query string filtering. To learn how to filter your data [read this](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/querying-data.md)

To read generally about the API and how to use it for your first time [go here](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/simplified-api-documentation.md#what-is-the-hakai-api).

The list of datasets (endpoints) can be found [here](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/simplified-api-documentation.md#endpoints).

In practice, I set the endpoint to the data table I'd like to retrieve and leave the default so I am returned the 20 most recent entries. I then look at the data file and remind myself what the exact names of the columns are so that I can construct the querystring filter so I can download the exact data I'd like.

Once I have my data set in my work environment, I'll make sure that all the variable classes are what I want them to be. Luckily, when `read_csv` reads in your file, it prints a summary of the name and type of each variable in your data frame. 

I always check: 

* that my Date column has the class 'Date'
* that categorical variables are stored as factors
* anything that is not a categorical variable is not stored as a factor but rather is numeric, character, or integer for example.

One of the most common problems of why you get errors in R is from R assigning incorrect variable classes. Often you want your variable to be of class numeric, or character, but it is assigned as a factor. This is extremely common and can be remedied by coercing your variable class, or using tools that are part of the tidyverse 

Once your data is imported into R and you are happy with the variable classes, it's time to start wrangling your data into a format you can visualize and model.

### Exploratory data analysis

This is where you start playing with the data. You will likely try many different summaries and visualizations while you seek a deeper understanding of your data. Read the [Exploratory Data Analysis]("http://r4ds.had.co.nz/exploratory-data-analysis.html") chapter in R for Data Science. 

I typically create an exploratory data analysis (EDA) script specifically for this, and save all my figures to an exploratory_figures  sub-directory using the function `ggsave(./figures/exploratory_figures/figure_1.png`. I use R Markdown files (.Rmd) for my EDA. This allows me to weave narrative explanations of what exploration I am doing in each separate code chunk. Writing this narrative about why you chose to explore a certain analysis or summary, and why it did or didn't work is key in documenting your analysis so you can come back to it, make sense of what you've done, and pick up where you left off. 

EDA allows me to try many different summaries or visualizations without having to worry about making the output perfect or pretty. This process will give you insight to your data as well as generate new questions and provoke new analyses. One excellent tool that can help you explore your data interactively is using [shiny apps](https://shiny.rstudio.com/). There's too much detail to focus on how to produce shiny apps here but suffice to say that creating interactivity with your data set is an excellent way to explore many different relationships between your data.

Exploratory analysis can probably go on forever depending on the complexity of your data. You will probably think of 1000 different ways to look at your data, but after some time you need to refocus on your original question and think carefully about what analyses and visualizations are best to communicate. Because you've been storing all of your EDA in and EDA script you've got a record of every path you went down while exploring your data. You can then cherry pick the analyses, plots, and summaries you want to include in your final scripts and simply copy and past them into a new .Rmd script.  


### Predict and Model

The type of statistical modelling you are going to do is dependent on the question you are asking, and is outside the scope of this guide. For some general guidelines on predicting and modeling read the section [Model]("http://r4ds.had.co.nz/model-intro.html") in R for Data Science.

### Communicate

What your final data product is going to be (summary report, figures for your manuscript, an interactive dashboard for a website), will dictate what your final scripts will be. As a baseline I recommend .Rmd as the final format because this gives you a lot of flexibility in terms of polished data products.

This is an area where R-Studio really shines. There are so many templates for communicating your analysis avaliable. Check out some of the [videos]("http://rmarkdown.rstudio.com/lesson-1.html") about RMarkdown and read the section in R for Data Science called [Communicate]("http://r4ds.had.co.nz/communicate-intro.html") to learn about the variety of ways you can communicate your analysis.


## More on Version Control

When you first start, using the git feature directly in R-Studio is more than adequate. If however, you'd like more features, then I recommend the following software and workflows.

### GitKraken

GitKraken is a graphical user interface that allows you to visualize your repositories branches and history. The Hakai IT team uses this software when developing tools, and it works quite well for backing up and monitoring the development of an analysis as well. Check out [GitKraken]("https://www.gitkraken.com/") to see if it's something you'd like to use. 

### Git Flow

Git Flow is a workflow that can be implemented in GitKraken by going to GitKraken > Preferences > GitFlow and then initialize gitflow. The workflow is generally good whether working individually or collaboratively. Some will argue that there's no such thing as working individually, especially when you consider that you're constantly collaborating with past-you, and future-you.

Some terms related to verison control and Git Flow:

* **Branching** - This is a core concept in Git Hub and version control in general -- The master branch must always remain stable and working. When you initialize git flow it creates a development branch. Essentially, you are creating an environment where you can try out new ideas without directly manipulating the stable 'master' version of your analysis. When you are happy with the changes you _push_ the development branch to the master branch. 

* **Commits** - Commits represent significant changes to your branch. I think of them as mini-milestones that add up to your complete analysis. Perhaps you're mini-milestone is to create a linear model of something. You can work locally and make save files on your computer while working on the linear model like you normally would. When you finish your linear model, this is a good time to make a commit to your branch. You must name each commit, and this should be descriptive. In this case 'fitted linear model' would be ideal.

To learn more about git flow check out [this article]("https://www.atlassian.com/git/tutorials/comparing-workflows#gitflow-workflow")


### Fork and Pull

If someone from outside the Hakai Institute GitHUB account wants to review your analysis and suggest changes, make additions, or generally collaborate with you, you can use a different workflow known as the [fork and pull model]("https://www.atlassian.com/blog/git/git-branching-and-forking-in-the-enterprise-why-fork").

* **Fork** - This is the process of copying someones repository and creating a fork in the development branch where you can work the forked copy all you want without affecting the original code. When you're happy with the changes you made to the repo, you can make a pull request.

* **Pull Requests** - I like to think of pull requests as an opportunity to review and assess external changes to the repo before you may decide to merge the fork back into the main branch. Something that helped clarify what a pull request is for me, was thinking about the difference between a push (write) request, and a pull (read) request. In the context of a shared repository, a pull request gives everyone that the repo is shared with a change to *read* your request to consider the sum of your mini-milestones. Pull requests should amount to the completion of a component of your project. See [here](https://help.github.com/articles/about-pull-requests/) for more info.

* **Merge** - Once a Pull Request has been sufficiently reviewed, discussed, and tested (deployed), the new code can be merged with the master branch, typically by the team owner, or maintainer.

* **Issues** - When working 'individually', an excellent way to create a TODO list for yourself is to create new 'Issues'. Again this could be something routine, like 'filter data to include only sampling events in the morning.' You can then leave yourself, and potential collaborators, a little note about why you're doing this which will be preserved in a timeline for future you, or collaborators, to inspect. Each issue in your repo is given a number, and future commits can be labelled 'fixes issue #45' and when that commit is wrapped up in a pull-request which is eventually merged to the master branch, the issue will be marked as resolved. When working collaboratively, issues can be created by whomever is reviewing your code, and the issue can be assigned to you to fix.

Unless you are added as an editor and part of the Hakai GitHUB team, you will likely be collaborating with Hakai analyses using the fork and pull method. To get comfortable using this technique I recommend completing the following steps:

1) __Issues__: (Need to make this repo public for this to be possible) Now that you have a Git Hub account, you should introduce yourself and let us know you're going to work through this material. To do that go to the issues tab in this repo and 'create a new issue' and type a message to introduce yourself. Tell us what program you are working on, and what your first data analysis project will likely be. Using issues is also a great way to ask questions about course material, or suggest improvements to the guide. During a normal data analysis, the issues function on Git HUB can be used as a way for reviewers to identify issues with your analysis or code, or simply as a to-do list for yourself or your team.

2) __Fork this repo__: Next, you should fork this repo. Think of forking a repo as creating a divergent copy of a set of files — like a fork in the road, two versions of the same original set of files can develop. You can edit your fork without comprimising the master version. Normally when you are happy with your changes and you want to suggest your edits to the owner you make A _pull request_. A pull request is simply your request to the owner of the master copy to pull your edits into the master version. This is a simple explanation of how collaborative editing works with Git HUB.  To successfully fork this repo and suggest edits simply go to [my repo here](https://github.com/Br-Johnson/Hakai-Data-Analyst) and click on the name of this chapter: 'new_analysts.Rmd', once it's open click on the pencil icon to suggest edits. Editing in this manner automatically forks the repo and makes a copy in your own list of repos where you will be able to see all the files that are available for others to view and edit. If you find an error anywher in this guide, edit it, and make a 'pull request' to suggest edits to me. This serves two purposes: i) the quality of this guide will be improved if you take time to edit chapters and contribute your insights on what didn't work for you; and ii) forking repos and making pull-requests is training you for the workflow you will use when contributing to colleagues' analyses in the very near future.  Now you can make all the edits you'd like and when your happy you can click on 'pull request' to send the updates back to me so I can review them and decide if I should accept your edits and merge them into the master version.

If you  aren't successful forking my repo, read [Chapter 24 of Jenny Bryan's Book; 'Fork a Repo'](http://happygitwithr.com/fork.html).
