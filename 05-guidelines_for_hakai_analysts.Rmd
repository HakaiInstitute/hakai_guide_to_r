# R Workflow Tips

## Create a new R-Studio Project

Generally I create a new project for every analysis I undertake, this makes things easy because the working directory remains constant amongst some other advantages.

To start a new project in R-Studio, go to File > New Project.

## File paths and the working directory

Often in an analysis you have to load or save files to a specific folder or file location on your computer. You should not use absolute paths to do this such as `write_csv(file_name, "C:Brett/documents/R projects/Hakai R Analyst/data/file_name.csv")`. The absolute path starts at the root of your computers specific file system, and other people will have different absolute paths on their computer depending on where they saved their files. So if you shared a script with an absolute path, your collaborator won't be able to run the script without headaches of changing the abosulute path. Fortunately, you can use relative paths. Relative paths don't go all the way back to the root of your file system. A relative file path in this example would be `"/Hakai R analyst/data/file_name.csv"`. Using relative paths makes the scripts or programs portable between computers. Defining where a relative path starts from can be done by setting your working directly, or by using the `here()` package.

A recommended method to work with relative file paths is using the `here()` package in conjunction with R-Studio projects. When you create a new R-Studio project, a .Rproj file is automatically created in the new folder that you created for the project. The `here()` package will automatically set your working directory to wherever your .Rproj file is saved. That means you can save a file like this: `write_csv(file_name, here("data", "file_name.csv"))`. Using `here()` means that if you access your collaborators folder where the .Rproj file is and they have been using relative paths using `here()`, the scripts should all just workâ€”no chaning working directories or absolute file paths.

### Create folder structure

I use a default folder structure for every analysis based on the files that are produced from every analysis. Using the project directory that you created your new R-Studio project, create these sub-folders within the project folder:

* raw data
* processed data 
* scripts
* figures

## Analysis Workflow

Here is a general workflow I typically adhere to, and could be adopted as a starting point from which individual analysts could modify.

### Import, pre-process & tidy data

I usually create at least two different scripts in any analysis, which helps me to compartmentalize the different steps of the analysis. I start with a pre-processing script that will read in and format all the different data sets I want to combine and analyze. This is where I make changes to data format that I want to apply to all subsequent uses of that data (so that I don't have to keep doing it in subsequent scripts).


1) __Create pre_process.R script__ In your newly created R Studio project, go to File > New File > R Script. Save it in the scripts sub-directory of your project directory.

2) Load packages

3) Import data:

__Method 1__:From spreadsheet you already have  

Most often you're going to want to read in files that are .csv files. These are comma separated value files and can be produced from excel or Google Sheets by saving your excel or Google Sheet file as a .csv file.

The first module of an analysis I produce is a plain .R script that loads in my .csv data file and save it in my R environment as a tibble, a tidy table, using the `new_tbl <- read_csv(here("data", "new_tbl.csv")` format. Before you read in a file, you should load the packages that we will be required for every analysis you conduct using the `library(tidyverse)` function. Note that you should not use the base R function `read.csv` but rather use the tidy-verse function `read_csv`. The base version will inevitably cause frustration due to incorrect variable class assignment for dates.

__Method 2__: From Google Drive

Using the googlesheets package in R is a pretty powerful tool and allows you to read googlesheets directly into R. This is great if your googlesheet is constantly changing as new data gets entered, and allows easy collaboration on data entry.

To read in a googlesheet:

`install.packages('googlesheets')`
`library(googlesheets)`

your_workbook <- gs_title('Name_of_your_workbook')
worksheet1 <- gs_read(your_workbook, ws = "sheet 1")

See this [documentation](https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html) on the googlesheets package for more info.

__Method 3__:From Hakai database

It is possible to get data from the Hakai database directly into R without having to leave R Studio. This is accomplished by interacting with an application programming interface (API) that was developed for downloading data from Hakai's data portal. Earlier in the Data Analysts Toolbox section we installed Hakai's API. Now we will use it to get Hakai data from the database.

Below is a quickstart example of how you can download some chlorophyll data. To be able to access Hakai data you must sign in with a Hakai email address that has the appropriate permissions to access the database. This authentication is done using a token. When you run the code below, a web URL will be displayed in the console. Copy and paste that URL into your browser. This should take to you a webpage that displays another web URL, this is your authentication token. Copy and paste the URL into the console in R Studio where it tells you to do so.

```
devtools::install_github("HakaiInstitute/hakai-api-client-r", subdir='hakaiApi')

library('hakaiApi')

# Get the api request client, run this line independently before the rest of the code
client <- hakaiApi::Client$new() # Follow stdout prompts to get an API token

# Make a data request for chlorophyll data
endpoint <- sprintf("%s/%s", client$api_root, "eims/views/output/chlorophyll?limit=50")
data <- client$get(endpoint)

# Print out the data
print(data)
```
By running this code you should see chlorophyll data in your environment. 
The above code can be modified to select different datasets other than chlorophyll and filter based on different logical parameters you set. This is accomplished by editing the text after the ? in `"eims/views/output/chlorophyll?limit=50"`.

The formula you set after the question mark is known as query string filtering. To learn how to filter your data [read this](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/querying-data.md). 

To read generally about the API and how to use it for your first time [go here](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/simplified-api-documentation.md#what-is-the-hakai-api).

The list of datasets (endpoints) can be found [here](https://github.com/HakaiInstitute/hakai-api/blob/master/docs/simplified-api-documentation.md#endpoints).

To avoid writing the query string yourself you can export the query string right from the [Hakai Data Portal](https://hecate.hakai.org/portal2/). When searching for the data you'd like in the Data Portal and after limiting your results to certain dates, or sites or whatever, you can click the `options` button in the top right of the screen and click Display API Query. You can copy and paste the string of characters starting at "eims/view/..." and paste that into your definition of an endpoint in R.

In practice, I set the endpoint to the data table I'd like to retrieve and leave the default so I am returned the 20 most recent entries. I then look at the data file and remind myself what the exact names of the columns are so that I can construct the querystring filter so I can download the exact data I'd like.

Once I have my data set in my work environment, I'll make sure that all the variable classes are what I want them to be. Luckily, when `read_csv` reads in your file, it prints a summary of the name and type of each variable in your data frame. 

I always check: 

* that my Date column has the class 'Date'
* that categorical variables are stored as factors
* anything that is not a categorical variable is not stored as a factor but rather is numeric, character, or integer for example.

One of the most common problems of why you get errors in R is from R assigning incorrect variable classes. Often you want your variable to be of class numeric, or character, but it is assigned as a factor. This is extremely common and can be remedied by coercing your variable class, or using tools that are part of the tidyverse 

Once your data is imported into R and you are happy with the variable classes, it's time to start wrangling your data into a format you can visualize and model.

### Exploratory data analysis

This is where you start playing with the data. You will likely try many different summaries and visualizations while you seek a deeper understanding of your data. Read the [Exploratory Data Analysis](http://r4ds.had.co.nz/exploratory-data-analysis.html) chapter in R for Data Science. 

I typically create an exploratory data analysis (EDA) script specifically for this, and save all my figures to an exploratory_figures  sub-directory using the function `ggsave(here("figures", "figure_1.png`)). I use R Markdown files (.Rmd) for my EDA. This allows me to weave narrative explanations of what exploration I am doing in each separate code chunk. Writing this narrative about why you chose to explore a certain analysis or summary, and why it did or didn't work is key in documenting your analysis so you can come back to it, make sense of what you've done, and pick up where you left off. 

EDA allows me to try many different summaries or visualizations without having to worry about making the output perfect or pretty. This process will give you insight to your data as well as generate new questions and provoke new analyses. One excellent tool that can help you explore your data interactively is using [shiny apps](https://shiny.rstudio.com/). There's too much detail to focus on how to produce shiny apps here but suffice to say that creating interactivity with your data set is an excellent way to explore many different relationships between your data.

Exploratory analysis can probably go on forever depending on the complexity of your data. You will probably think of 1000 different ways to look at your data, but after some time you need to refocus on your original question and think carefully about what analyses and visualizations are best to communicate. Because you've been storing all of your EDA in and EDA script you've got a record of every path you went down while exploring your data. You can then cherry pick the analyses, plots, and summaries you want to include in your final scripts and simply copy and past them into a new .Rmd script.  

### Predict and Model

The type of statistical modelling you are going to do is dependent on the question you are asking, and is outside the scope of this guide. For some general guidelines on predicting and modeling read the section [Model](http://r4ds.had.co.nz/model-intro.html) in R for Data Science.

### Communicate

What your final data product is going to be (summary report, figures for your manuscript, an interactive dashboard for a website), will dictate what your final scripts will be. As a baseline I recommend .Rmd as the final format because this gives you a lot of flexibility in terms of polished data products.

This is an area where R-Studio really shines. There are so many templates for communicating your analysis avaliable. Check out some of the [videos](http://rmarkdown.rstudio.com/lesson-1.html) about RMarkdown and read the section in R for Data Science called [Communicate](http://r4ds.had.co.nz/communicate-intro.html) to learn about the variety of ways you can communicate your analysis.

