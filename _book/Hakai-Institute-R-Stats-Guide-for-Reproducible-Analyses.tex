\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Hakai Institute R Stats Guide for Reproducible Analyses},
            pdfauthor={Brett Johnson},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Hakai Institute R Stats Guide for Reproducible Analyses}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Brett Johnson}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Latest update: 2018-12-18}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{0}
\tableofcontents
}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Once you build this book, you can serve the book to auto update in the viewer window by removing the hashtag in the code line below}

\CommentTok{#bookdown::serve_book(dir = ".", output_dir = "_book", preview = TRUE, in_session = TRUE)}
\end{Highlighting}
\end{Shaded}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics[width=9.97in]{images/r_stats_red}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

One of Hakai's core themes that cross-cuts each
\href{https://www.hakai.org/research}{research axis} is `Big-Data and
Modelling'. Our ability to gain insight from our long-term ecological
data sets is limited only by our capacity to integrate, and digest data.
To that end, this guide was developed to serve as a foundation from
which to build the internal technical capacity --- using a coherent and
systematic approach --- to turn data into insight, understanding, and
ultimately knowledge about the ecosystems we study.

Through this guide you can learn data wrangling skills, R programming
basics, project workflow and organization, mapping in R, reproducible
research techniques, and collaborative analysis development in an
open-science context.

\chapter{Introduction}\label{introduction}

\section{Filling in the Gaps}\label{filling-in-the-gaps}

Most University courses that teach statistics and data analysis focus on
teaching statistical techniques but they pay little attention to the
tools, workflows, and data wrangling skills required to actually conduct
an analysis from start to finish. Questions that often remain
un-answered include:

\begin{itemize}
\tightlist
\item
  What is an efficient workflow?
\item
  How do I access and import data?
\item
  How do I clean and manipulate my data into a format to analyze?
\item
  How can I re-run my analysis in case I get new data or someone else
  wants to run my code?
\item
  How can I collaborate on this analysis?
\item
  How can I get my analysis into a format for someone to meaningfully
  conduct peer-review?
\item
  How do I efficiently produce a professional report or other artifact
  of my analysis to communicate results?
\end{itemize}

This guide aims to bridge this gap by taking you through the steps to
develop an analysis. Learning R can be a very frustrating and difficult
experience. Know that we have all suffered severely in this way, but
that this guide will hasten you through the frustration. No computer
programming experience is required to work through this guide.

\section{Tidyverse}\label{tidyverse}

As the R community iterated over ideas of how the programming language
works at a very base level over the last couple of decades, we are left
with a litany of methods and programming syntax. There are typically at
least half a dozen ways to write a chunk of code to reach the same
desired result in R. This, in part, has given R the reputation as being
difficult to learn.

This guide aims to address the `thousand and one way of doing things'
problem in R by focusing on the recent development of packages that form
a simple, elegant, and coherent grammar of data analysis. This
collection of packages and methods is known as the `tidyverse',
developed in large part by the Chief Scientist of R-Studio, Hadley
Wickham, and has been widely adopted as the way forward for academic
applications of the R language.

The objectives of this book are to serve as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  A guide to install, set-up, and become familiar with analysis
  development tools; R, R-Studio, Git and Git Hub.
\item
  Best-practice guidelines to develop reproducible, accurate, and
  collaborative analyses in R-Studio.
\item
  Provide code templates for analyses and work common to Hakai employees
  and affiliates.
\end{enumerate}

\section{Acknowledgements}\label{acknowledgements}

Much of this document refers you to material that others have worked
very hard to make. I simply point to these resources in an order that
makes sense and seems systematic to me. Many thanks to the following
people for making this possible:

\begin{itemize}
\tightlist
\item
  Dr.~Jenny Bryan, Data Science Professor at UBC, RStudio Employee.
  \href{https://twitter.com/JennyBryan}{Twitter},
  \href{https://github.com/jennybc}{GitHUB}.
\item
  Dr.~Roger Peng, Professor of Bio-statistics at Johns Hopkins
  University. \href{http://www.biostat.jhsph.edu/~rpeng/}{Website},
  \href{https://github.com/rdpeng}{GitHUB}.
\item
  Dr.~Hadley Wickham, Chief Scientist at R-Studio.
  \href{http://hadley.nz/}{Website},
  \href{https://twitter.com/hadleywickham}{Twitter},
  \href{https://github.com/hadley}{GitHUB}.
\item
  Dr.~Hillary Parker, Data Scientist at Stitch-fix.
  \href{https://twitter.com/hspter}{Twitter}
\end{itemize}

\section{Contributors}\label{contributors}

A number of people have make important contributions directly to this
guide. This is highly recommended and can be done by submitting issues
\href{https://github.com/HakaiInstitute/hakai_guide_to_r/issues}{here}
at the repository where this guide is hosted. Issues can be bugs, typos,
or ideas for additional material. If you're already a GitHUB guru, feel
free to fork the repositoory, make some changes or additions, and submit
a pull-request!

Thanks to the following people for significant contributions:

\begin{itemize}
\tightlist
\item
  Dr.~Daniel Okamoto
\item
  Dr.~Jen Burt
\item
  Matt Whalen
\end{itemize}

\chapter{General Principles}\label{general-principles}

New ideas about what makes a good data analysis are emerging. With data
being so readily available in vast quantities, analyzing data using out
of date methods --- such as Microsoft excel --- can quickly become
overwhelming, not reproducible, error-prone, and difficult to assess for
reliability.

Much of the progress in terms of `developing analyses' has been made in
the field of bio-statistics due to the high volume of genomic data that
researchers deal with. One of the most concerning examples of what can
go wrong with an analysis, is from the field of genomics and cancer
treatments. In the
\href{http://www.cbsnews.com/news/deception-at-duke-fraud-in-cancer-care/}{`Duke
Scandal'}, researchers made mistakes in their data analysis, that were
extremely difficult to track, and resulted in patients receiving the
wrong cancer treatment. This is an extreme example that affected peoples
lives directly. I would argue, that the work that we do at Hakai,
analyzing ecological data, has much broader implications and should be
treated with an equal degree of discretion.

Some important concepts in defining a good data analysis are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Reproducible Research,
\item
  Open Science Collaboration
\end{enumerate}

These two concepts bring together a very modern way of conducting
science that are beneficial in the following ways:

\begin{itemize}
\tightlist
\item
  An additional peer review process becomes possible in the development
  of your analyses---peers can review how you conducted your analysis
\item
  You can reduce your work in the long run by being able to reproduce
  your own analyses after you've long forgotten the details of how they
  were conducted
\item
  If you weave a narrative text into your R code you'll be able to
  understand what you were thinking at a later time when you revisit it
\item
  You can meaningfully collaborate
\item
  You can show your peers that you have nothing to hide in your analytic
  methods, immediately increasing the reliablilty of your work
\item
  You can share your analyses in hopes that others will improve the
  quality of your analysis by offering their insight
\end{itemize}

\section{Reproducible Research}\label{reproducible-research}

A big concern with modern scientific studies is the inability to
reproduce study findings. As experimental and analytic methods become
increasingly complex, published methods often lack the detail required
to be able to reproduce the study. As data analysts, it is our
responsibility to include in our analyses sufficient notation to
facilitate deep understanding of what was done.

If your study finds something very interesting, people are going to want
to know how you came to your conclusion. The gold standard for verifying
a study is to independently replicate it. However, before investing the
huge amount of resources required to replicate a study independently, a
better place to start is to reproduce the study findings using their
data and analysis.

In order for your study and your analysis to be reproducible and to be
viewed as trustworthy, you need to be able to provide the data, the
scripted code you used to clean, summarize, model, and visualize that
data, and then a reviewer has to be able to run that same code and see
the same results. This level of transparency allows a reviewer to look
very closely at how you conducted your analysis. This adds an additional
step in the peer review process which has not previously been possible
with un-scripted analyses.

A simple example of how easy it is to break the reproducibility chain
comes from working with your data in excel. By simply deleting some
values that looked to be outliers, without recording anywhere that you
did that, or why you did that, you have effectively broken the
reproducibility chain. Another person could not receive the raw data and
come to the same conclusions as you did --- their results would be
different because of the missing data.

Some academic journals, such as the Journal of Bio-statistics, are
taking proactive steps by adopting a
\href{https://academic.oup.com/biostatistics/article/10/3/405/293660/Reproducible-research-and-Biostatistics\#3746779}{reproducibility
policy} for article submissions. The journal of Nature has taken a
stance on reproducibility and has published several articles in a
special called
\href{https://www.nature.com/news/reproducibility-1.17552}{Challenges in
Irreproducible Research}. If you google reproducible research you will
find many initiatives that are attempting to solve this important
problem.

Most often, the peer-reviewer or collaborator that you will work with is
`future-you'. Scripting reproducible analyses with embedded narrative
allows future-you to understand what past-you was thinking. This, in
turn, saves you a lot of time, and allows you to build upon your
previous work. The idea of embedding your own narrative, or adding
comments to your code, introduces the idea of \emph{literate
programming}. Weaving together human-readable narrative text that
explains what your computer code is doing and why you decided to do it,
greatly increase the quality of your work.

For some comic relief on the follies of working with someone who doesn't
use reproducible research methods watch
\href{https://www.youtube.com/watch?v=N2zK3sAtr-4\&feature=youtu.be}{this
youtube video}

\section{Open Science Collaboration}\label{open-science-collaboration}

\subsection{Peer Review}\label{peer-review}

Open science collaboration establishes an additional peer-review step in
the scientific process. By making your analysis public using a
\href{https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control}{distributed
version control system} like GitHub you open up the possibility for
on-going peer-review of your analysis, as well as the the opportunity
for not only your immediate colleagues, but also experts in your field
to contribute in a meaningful and formalized way.

Using consistent formatting for writing your code, makes it easier for
everyone to interpret. I recommended the
\href{http://style.tidyverse.org}{The tidyverse style guide}. Have a
read through the guide and try to implement in for your next script.

\chapter{Data Analyst's Toolbox}\label{data-analysts-toolbox}

\section{Install R and R Studio}\label{install-r-and-r-studio}

R is the statistical programming language that R Studio provides and
interface to. I think of R Studio as being the front-end user interface
to the coding language R which works in the background. For this all to
work you you need to download two things to start; R and R Studio.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Install R}: Go to \url{https://cran.r-project.org/} and follow
  the link to the download for your operating system.
\item
  \textbf{Install R Studio}: Go to
  \url{https://www.rstudio.com/products/rstudio/download/} to download R
  studio.
\end{enumerate}

Now that you have downloaded the tools you need, lets get familiar first
with R-Studio and then the R programming language.

\subsection{Wet Your Feet}\label{wet-your-feet}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{R Studio Familiarization}: Watch
  \href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-part-1/}{this
  R-Studio Tutorial} for a nice introduction to the software.
\item
  \textbf{Start Plotting}: Now that you're familiar with the software,
  lets get your feet wet in R and R-Studio. I recommend you start making
  some plots by working through the following exercises:
  \href{http://r4ds.had.co.nz/data-visualisation.html}{Section 3.1 to
  3.4} in Hadley Wickham's R for Data Science Book.
\item
  \textbf{Projects Workflow}: Watch R-Studio tutorial:
  \href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-managing-change-part-1/}{Projects
  and R Studio}
\item
  \textbf{Install Hakai's Application Programming Interface} This API is
  installed directly in R-Studio and allows you to download data from
  the Hakai Database. Open up R Studio and in the console window copy
  and paste the following code and hit enter:
\end{enumerate}

\begin{verbatim}
install.packages('devtools')
install.packages('tidyverse')
library('devtools')
library('tidyverse')

devtools::install_github("HakaiInstitute/hakai-api-client-r", subdir='hakaiApi')
\end{verbatim}

\chapter{Intro to R}\label{intro-to-r}

In this section you will learn the basics of data wrangling in R. Data
wrangling is the process of importing, tidying, and transforming your
data into a format that you can visualize and model it.
\href{http://vita.had.co.nz/papers/tidy-data.pdf}{Tidy data} is a
consistent data format that, when followed closely, dramatically reduces
the mundane data formatting that is necessary otherwise. The basic
concept is that every row is one observation and every column is a
variable. For how simple it sounds, creating tidy data sets takes some
time to get the hang of initially, but is essential for data wrangling
in the tidyverse.

\section{Primary Resource}\label{primary-resource}

The `bible for a new generation of Data Scientists' is Hadley Wickham
and Garrett Grolemund's Book: \href{http://r4ds.had.co.nz/}{R For Data
Science}. This book presents a modern approach to data analysis and
leads you to master the tidyverse; a combination of R packages and a
well thought out and systematic approach to ``import, tidy, transform,
visualize, and model data.'' Using the tidyverse as a foundation for
your coding replaces the `thousand and one ways' of doing things in R
into a modern and concise grammar of data analysis development.

\section{Data Transformation}\label{data-transformation}

The most important set of tools for data wrangling that you will use
constantly are from the package \texttt{dplyr}. They allow you to solve
almost all of the data manipulation problems you will encounter. Those
who are able to quickly manipulate their data into a format they can
visualize and model, will explore their data the most and therefore
learn the most. I'd recommend learning these six data wrangling
functions off-by-heart:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{filter()} to pick observations (rows) based on their values
\item
  \texttt{select()} to pick variables (columns) based on the names
\item
  \texttt{arrange()} to reorder the rows of your data set
\item
  \texttt{mutate()} to create new variables (columns) as a function of
  existing variables
\item
  \texttt{summarize()} to collapse many values down to a summary
\item
  \texttt{group\_by()} to summarize based on the various groups of a
  variable
\end{enumerate}

To learn how to put this in to practice I recommend reading through the
chapter called \href{http://r4ds.had.co.nz/transform.html}{Data
Transformation} in R for Data Science and working through all the
exercises. This will be a bit of a learning curve but it's time to dive
in!

Starting at the Data Transformation chapter assumes you have some very
basic knowledge of R already. If that's not the case then I recommend
starting at the
\href{http://r4ds.had.co.nz/workflow-basics.html}{Workflow: Basics}
chapter.

\section{Tidying Your Data}\label{tidying-your-data}

Tidy data is best explained by a quote from
\href{http://www.jstatsoft.org/v59/i10/paper}{Hadley Wickham's paper} in
the Journal of Statistical Software:

``Tidy data sets are easy to manipulate, model and visualize, and have a
specific structure: each variable is a column, each observation is a
row, and each type of observational unit is a table.''

The basic tenets that make a data set tidy are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable must have its own column
\item
  Each observation must have its own row
\item
  Each value must have its own cell
\end{enumerate}

All of the data stored in Hakai's database is designed to be tidy, but
sometimes you will receive data that is not in a tidy format. The most
common problem I run into is that not every row is one observation, it
is often many more than one because observations are stored as
variables. To get your data into a tidy format you have to spread out
the observations into different rows and create new variables that were
once values. Luckily, the package \texttt{tidyr} has several functions
that make this possible.

To get a really strong handle on what tidy data is, and to start
practising getting data in to a tidy format I recommend working through
the chapter on \href{http://r4ds.had.co.nz/tidy-data.html}{Tidying Data}
in the R for Data Science book.

\section{Visualizing Data}\label{visualizing-data}

In the last chapter you had a teaser of what is possible in terms of
making some nice looking plots when you worked through the first few
sections in the Data Visualization chapter in R for Data Science. To see
what else is possible and to learn the principles of the layered grammar
of graphics, I recommend you work through the remaining sections in the
Data Visualization Chapter picking up where you left of at the
\href{http://r4ds.had.co.nz/data-visualisation.html\#facets}{sub-section
on facets}

By the end of this you should have a really solid foundation from which
to begin wrangling and visualizing your own data. In the next chapter
I'll provide some gerenal principles for developing analyses.

\section{Learning Objectives}\label{learning-objectives}

Before moving on to the next chapter, work through these chapters
mentioned above and complete their respective exercises:

\begin{itemize}
\tightlist
\item
  \href{http://r4ds.had.co.nz/workflow-basics.html}{Workflow: Basics}
\item
  \href{http://r4ds.had.co.nz/transform.html}{Data Transformation}
\item
  \href{http://r4ds.had.co.nz/tidy-data.html}{Tidying Data}
\item
  \href{http://r4ds.had.co.nz/data-visualisation.html\#facets}{Data
  Visualization}
\end{itemize}

I highly reccommend working through all the chapters in this book at
some point, but these first few chapters are a good start.

\chapter{Start Your Project}\label{start-your-project}

\section{Create a new R-Studio
Project}\label{create-a-new-r-studio-project}

I create a new project for every analysis I undertake. Using R Studio
Projects is one of the key benefits of using R Studio because it makes
it easier to organize your projects, makes sharing projects easier,
makes loading data easier, and improves reproducibility.

To start a new project in R-Studio, go to File \textgreater{} New
Project.

\section{File Paths and the Working
Directory}\label{file-paths-and-the-working-directory}

Often in an analysis you have to load or save files to a specific folder
or file location on your computer. You should not use absolute paths to
do this such as
\texttt{write\_csv(file\_name,\ "C:Brett/documents/R\ projects/Hakai\ R\ Analyst/data/file\_name.csv")}.
The absolute path starts at the root of your computer's specific file
system, and other people will have different absolute paths on their
computer depending on where they saved their files. So if you shared a
script with an absolute path, your collaborator won't be able to run the
script without headaches of changing the abosulute path. Fortunately,
you can use relative paths. Relative paths don't go all the way back to
the root of your file system. A relative file path in this example would
be \texttt{"/Hakai\ R\ analyst/data/file\_name.csv"} because that's
where this R Project folder starts. Using relative paths makes the
scripts or programs portable between computers.

\subsection{\texorpdfstring{The \texttt{here()}
package}{The here() package}}\label{the-here-package}

To avoid having to set your working directory completely, a recommended
method to work with relative file paths is using the \texttt{here()}
package in conjunction with R-Studio projects. When you create a new
R-Studio project, a .Rproj file is automatically created in the new
folder that you created for the project. The \texttt{here()} package
will automatically set your working directory to wherever your .Rproj
file is saved. That means you can save a file like this:
\texttt{write\_csv(file\_name,\ here("data",\ "file\_name.csv"))}. Using
\texttt{here()} means that if you access your collaborators folder where
the .Rproj file is and they have been using relative paths using
\texttt{here()}, the scripts should all just work---no chaning working
directories or absolute file paths.

\subsection{Create folder structure}\label{create-folder-structure}

I use a default folder structure for every analysis based on the files
that are produced from every analysis. Using the project directory that
you created your new R-Studio project, create these sub-folders within
the project folder:

\begin{itemize}
\tightlist
\item
  data
\item
  data
\item
  scripts
\item
  figures
\end{itemize}

\section{Importing Data}\label{importing-data}

Here is a general workflow I typically adhere to, and could be adopted
as a starting point from which individual analysts could modify.

I usually create at least two different scripts in any analysis, which
helps me to compartmentalize the different steps of the analysis. I
start with a data wrangling script that will read in and format all the
different data sets I want to use, and then write them to my data folder
to be read from the actual analysis script.

\textbf{Create a data\_wrangle.R script} In your newly created R Studio
project, go to File \textgreater{} New File \textgreater{} R Script.
Save it in the scripts sub-directory of your project directory.

\subsection{From spreadsheets}\label{from-spreadsheets}

Most often you're going to want to read in files that are .csv files.
These are comma separated value files and can be produced from excel or
Google Sheets by saving your excel or Google Sheet file as a .csv file.

The first module of an analysis I produce is a plain .R script that
loads in my .csv data file and save it in my R environment as a tibble,
a tidy table, using the
\texttt{new\_tbl\ \textless{}-\ read\_csv(here("data",\ "new\_tbl.csv")}
format. Before you read in a file, you should load the packages that we
will be required for every analysis you conduct using the
\texttt{library(tidyverse)} function. Note that you should not use the
base R function \texttt{read.csv} but rather use the tidy-verse function
\texttt{read\_csv}. The base version will inevitably cause frustration
due to incorrect variable class assignment for dates.

\subsection{From Google Drive}\label{from-google-drive}

Using the googlesheets package in R is a pretty powerful tool and allows
you to read googlesheets directly into R. This is great if your
googlesheet is constantly changing as new data gets entered, and allows
easy collaboration on data entry.

To read in a googlesheet:

\texttt{install.packages(\textquotesingle{}googlesheets\textquotesingle{})}
\texttt{library(googlesheets)}

\begin{verbatim}
your_workbook <- gs_title('Name_of_your_workbook')

worksheet1 <- gs_read(your_workbook, ws = "sheet 1")
\end{verbatim}

See this
\href{https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html}{documentation}
on the googlesheets package for more info.

\subsection{From Hakai Data Portal
API}\label{from-hakai-data-portal-api}

It is possible to download data from the Hakai EIMS Data Portal database
directly from R Studio. This is accomplished by interacting with an
application programming interface (API) that was developed for
downloading data from Hakai's data portal.

Below is a quickstart example of how you can download some chlorophyll
data. Run the code below one line at a time. When you run the
\texttt{client\ \textless{}-\ ...} line a web URL will be displayed in
the console. Copy and paste that URL into your browser. This should take
to you a webpage that displays another web URL, this is your
authentication token that permits you access to the database. Copy and
paste the URL into the console in R where it tells you to do so.

\begin{verbatim}
# Run this first line only if you haven't installedt the R API before
devtools::install_github("HakaiInstitute/hakai-api-client-r", subdir='hakaiApi')

library('hakaiApi')

# Run this line independently before the rest of the code to get the API authentication
client <- hakaiApi::Client$new() # Follow stdout prompts to get an API token

# Make a data request for chlorophyll data
endpoint <- sprintf("%s/%s", client$api_root, "eims/views/output/chlorophyll?limit=50")
data <- client$get(endpoint)

# Print out the data
print(data)
\end{verbatim}

By running this code you should see chlorophyll data in your
environment. The above code can be modified to select different datasets
other than chlorophyll and filter based on different logical parameters
you set. This is accomplished by editing the text after the ? in
\texttt{"eims/views/output/chlorophyll?limit=50"}.

The formula you set after the question mark is known as query string
filtering. To learn how to filter your data
\href{https://github.com/HakaiInstitute/hakai-api/blob/master/docs/querying-data.md}{read
this}.

To read generally about the API and how to use it for your first time
\href{https://github.com/HakaiInstitute/hakai-api/blob/master/docs/simplified-api-documentation.md\#what-is-the-hakai-api}{go
here}.

If you don't want to learn how to write a querystring yourself there is
an option to just copy and paste the querystring from the
\href{https://hecate.hakai.org/portal2/}{EIMS Data Portal}. Use the
portal to select the sample type, and dates and sites you'd like to
download as you normally would. To copy the querystring go to the top
right of the window where it says Options and click `Display API query'.
You can copy that string in to your endpoint definition in R. Just be
sure to copy that string starting from \texttt{eims/views/...},
excluding \texttt{https://hecate.hakai.org/api/} and then paste that
into the definitions of your endpoint and surround that string with
single quotes ie:
\texttt{endpoint\ \textless{}-\ sprintf("\%s/\%s",\ client\$api\_root,\ \textquotesingle{}eims/views/output/chlorophyll?date\textgreater{}=2016-11-01\&date\textless{}2018-11-20\&work\_area\&\&\{"CALVERT"\}\&site\_id\&\&\{"KC13","MG4"\}\&survey\&\&\{"KWAK","MACRO\_MGEO"\}\&limit=-1\textquotesingle{}}

Make sure to add \&limit=-1 at the end of your query string so that not
only the first 20 results are downloaded, but rather everything matching
your query string is downloaded.

The page documenting the API usage can be found
\href{https://hakaiinstitute.github.io/hakai-api/}{here}

Once you're happy with the formatting and filtering you've applied to
your data make sure to write a new data file that you can read in from
your separate analysis script.

\texttt{write\_csv(here("data",\ "my\_data.csv"))}

\subsection{Analysis}\label{analysis}

Now create a new script for your analysis. I like to use R Markdown
files (.Rmd) for my anlaysis which allows me to weave narrative
explanations of what anlaysis I am doing in each separate code chunk.
Read in the data you created in the \texttt{data\_wrangling.R} script.

\texttt{read\_csv(here("data",\ "my\_data.csv"))}

Begin analyzing your data!

\subsection{Communicate}\label{communicate}

What your final data product is going to be (summary report, figures for
your manuscript, an interactive dashboard for a website), will dictate
what your final scripts will be. As a baseline I recommend .Rmd as the
final format because this gives you a lot of flexibility in terms of
polished data products.

This is an area where R-Studio really shines. There are so many
templates for communicating your analysis avaliable. Check out some of
the \href{http://rmarkdown.rstudio.com/lesson-1.html}{videos} about
RMarkdown and read the section in R for Data Science called
\href{http://r4ds.had.co.nz/communicate-intro.html}{Communicate} to
learn about the variety of ways you can communicate your analysis.

\chapter{Version Control and
Collaboration}\label{version-control-and-collaboration}

\emph{Version control} is an additional level of saving your files. The
old school method of version control is to have multiple versions of the
same file on your computer with different dates or initials to identify
the version you want to work on (eg.
fishy\_analysis\_V9\_2017\_05\_11\_BJ\_edits.R, etc\ldots{}) --- this is
what version control using Git and GitHub is aims to simplify.

There are two methods that are recommended for version controlling your
files: Google Drive and Git/GitHUB. Each has their own advantages and
disadvantages and can be used under different circumstances.

\section{Google Drive}\label{google-drive}

\subsection{Version Control}\label{version-control}

Googlesheets, Google Docs, Google Slides, has for me, replaced Excel,
Word, and Powerpoint. The ability to collaborate in real time on the
same file as someone else has great advantages and solved many of the
issues with version controlling edits to a source file. The version
controlling is taken care of for you and you can look into the version
history to see exactly what and when changes were made and by whom.
Essentially everyone works off the master copy (or using git jargon ---
commits to the master branch). The version history of any Google file
(Sheet, Doc, Slides) can found on the browser version of Google Drive by
opening the document or sheet and selecting \texttt{file} \textgreater{}
\texttt{version\ history}. If you'd like to make a specific version more
memorable before or after some significant change, you can `name the
version'. The actual name of the file does not change, however the
version will receive a name in the version history. This allows you to
better keep track of important changes to a file.

Keeping data files and R scripts in Google Drive for version controlling
isn't quite as slick as it is for google products (sheets etc.). It
does, however, certainly have some advantages compared to emailing
colleuages different versions of files back and forth, and renaming them
each time.

Everytime you save a file that is located in a Google Drive folder on
your computer, it is backed up in the cloud and the version of that file
is saved. Managing the version history of files other than Google
Sheets, Docs, Slides etc, is a little different. Only the most recent
versions of your file will be saved unless you right-click the file in
the web version of Google Drive and check the box that says `Keep
Forever'.

Google Drive is a simplified and intuitive version control system that
can work well under some scenarios. Google Drive falls short, however,
when you and a colleauge are working in a collaborartive development of
an .R file. Maybe you are both working on the same .R file to build a
complicated analysis, and your part depends on their part so you have to
do it in the same script. If there is a chance you could be working on
the same script at the same time, then Google Drive will fail you. If
you both opened the .R file from Google Drive at the same time and one
person finished making their edits after the other, the first persons
edits would be erased as the second person uploads their version. This
is one of the major problems that Git and GitHUB solves.

\section{Git and GitHUB}\label{git-and-github}

While peer-review permits the scientific merit of your analyses to be
assesed, code-review permits a colleague to assess your analyses for
coding errors. Developing analyses is difficult and error prone. Using
the \emph{distributed version control} system
\href{https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control}{GitHUB},
you are able to track changes to your code over time and enable others
to suggest edits for your review, leaving you with a history of exactly
what was added when and by whom --- this is version control. The
distributed aspect of `distributed version control' means that multiple
people can access the version control through your \emph{repository} of
all the files of your analysis (and their previous versions). You can
make your repository (repo) private or public. By acessing your repo,
which is hosted on GitHub, others can meaningfully collaborate, conduct
peer-review, and code-review. Git is the \emph{local} version control
system running in the background on your computer while GitHub is the
remote user interface for saving, tracking, and sharing updated versions
of your files that are hosted on a remote server. Git and GitHub are
integral for tracking the evolution of a set of files, and the
development of your analysis.

The additional `save' that comes from version control using git is known
as a \emph{commit}. You save your files like you normally would, but
every once in a while you commit your files as an official version to be
remembered. A commit can be thought of as a bullet point in the to do
list of your analysis, and each commit you make must be accompanied by a
message. For example; `read in data and tidy it up', or `remove
observations from non-standard sampling event, and re-fit GLM'. Git
tracks the commits you make in R-Studio locally on your own computer.
When you are ready for a series of commits to be made public, you
\emph{push} your commits to your \emph{remote} repository at GitHub.

If you're interested in learning why version control is important, I
encourage you to skim
\href{http://happygitwithr.com/big-picture.html\#why-git}{Chapter 1;
`Why Git? Why GitHub' from Jenny Bryan's Book}. I recommend referring
back to this book whenever you have a question about using git and
GitHub with R-Studio. You can also watch
\href{https://www.youtube.com/watch?v=FyfwLX4HAxM}{this video} for an
introduction to Git and GitHub.

\subsection{Step-by-step Install
Git/GitHub}\label{step-by-step-install-gitgithub}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Setup a GitHub Account}: First, you're going to want to
  \href{https://github.com}{sign up for a GitHub account}
\item
  \textbf{Install and set-up Git}: Installing Git locally and getting
  Git to communicate to your remote GitHub website account, and then
  getting them to talk with R Studio takes a number of steps to
  complete\ldots{}This next part can be painful to work through, but it
  is 100 \% necessary and well worth it in the long-run. Install Git and
  GitHub and get them talking to R-Studio using
  \href{http://happygitwithr.com/install-git.html}{Jenny Bryan's Guide},
  follwow along wth chapter 7, 8, 10, and 13.
\item
  \textbf{Put this all in context}: To learn how to use Git, GitHub and
  R-Studio I recommend watching the
  \href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-managing-part-2/}{R-Studio
  Essentials tutorial; GitHub and R-Studio}.
\end{enumerate}

If you are working directly for Hakai, you should contact the Ecological
Information Management department:
\href{mailto:eims@hakai.org}{\nolinkurl{eims@hakai.org}}, to request
that you be brought on to the Hakai GitHub team so that your work is
stored in the Hakai repositories rather than on your personal account.

\subsection{Start an R-Studio project under version
control}\label{start-an-r-studio-project-under-version-control}

\subsubsection{Set up a repo on GitHUB}\label{set-up-a-repo-on-github}

If you are working on a specific program, there's a good chance your
program already has a central GitHUB repo set up for all your programs
analyses. If you've been asked to conduct some data summaries or
analyses you should talk to your PI about storing the code you write in
your program's repo. If your program doesn't have a repo, you're going
to want to set one up. Make sure you talk to your PI and someone from
Hakai EIMS about this.

Whether you're setting up a repo on your own GitHub account or the Hakai
one, your going to want to first create the repo, and then create your
R-Studio project by following these instructions. If the repo you want
to use already exists, then just go straight to the next section: `Start
a new R-Studio project.'

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Go to \url{https://github.com} and login.
\item
  Create a new repository on your own account, or on the Hakai account
  depending on the scenario.
\item
  Call it something-descriptive-but-concise.
\item
  Make it private to start (talk to your PI or IT about making it
  Public) and click yes to initialize with a README.
\item
  Click the big green button ``Create repository.''
\item
  Copy the HTTPS clone URL to your clipboard via the green ``Clone or
  Download'' button. You'll need this to link the repo with R-Studio.
\end{enumerate}

\subsubsection{Start a new R-Studio
project}\label{start-a-new-r-studio-project}

To link the gitHUB repo to your Google drive folder and a new project in
R-Studio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Open R-Studio and go to File \textgreater{} New Project \textgreater{}
  Version Control \textgreater{} Git. In the ``repository URL'' paste
  the URL of the Git Hub repository you just made. Under project
  directory name, type in the name of whatever this analysis is, for
  example ``soft sediment spp diversity'' --- this will be the name of
  the analysis' directory. Under `Create project as a sub-directory of'
  navigate to your programs Google Drive folder for analyses. Click
  ``Create Project''.
\item
  Go to Tools in the menu bar, and navigate to Project Options. Click
  Packrat, and then check the box `Use Packrat with this project'. This
  may take a few minutes to initialize, but is important to make your
  analyses reproducible in the long term.
\end{enumerate}

This should download the README.md file that we created on GitHub in the
previous step. Look in R Studio's file browser pane for the README.md
file."

\subsection{GitKraken}\label{gitkraken}

When you first start, using the git feature directly in R-Studio is more
than adequate. If however, you'd like more features, then I recommend
the following software and workflows.

GitKraken is a graphical user interface that allows you to visualize
your repositories branches and history. The Hakai IT team uses this
software when developing tools, and it works quite well for backing up
and monitoring the development of an analysis as well. Check out
\href{https://www.gitkraken.com/}{GitKraken} to see if it's something
you'd like to use.

\subsubsection{Git Flow}\label{git-flow}

Git Flow is a workflow that can be implemented in GitKraken by going to
GitKraken \textgreater{} Preferences \textgreater{} GitFlow and then
initialize gitflow. The workflow is generally good whether working
individually or collaboratively. Some will argue that there's no such
thing as working individually, especially when you consider that you're
constantly collaborating with past-you, and future-you.

Some terms related to version control and Git Flow:

\begin{itemize}
\item
  \textbf{Branching} - This is a core concept in Git Hub and version
  control in general -- The master branch must always remain stable and
  working. When you initialize git flow it creates a development branch.
  Essentially, you are creating an environment where you can try out new
  ideas without directly manipulating the stable `master' version of
  your analysis. When you are happy with the changes you \emph{push} the
  development branch to the master branch.
\item
  \textbf{Commits} - Commits represent significant changes to your
  branch. I think of them as mini-milestones that add up to your
  complete analysis. Perhaps you're mini-milestone is to create a linear
  model of something. You can work locally and make save files on your
  computer while working on the linear model like you normally would.
  When you finish your linear model, this is a good time to make a
  commit to your branch. You must name each commit, and this should be
  descriptive. In this case `fitted linear model' would be ideal.
\end{itemize}

To learn more about git flow check out
\href{https://www.atlassian.com/git/tutorials/comparing-workflows\#gitflow-workflow}{this
article}

\subsection{Fork and Pull}\label{fork-and-pull}

If someone from outside the Hakai Institute GitHUB account wants to
review your analysis and suggest changes, make additions, or generally
collaborate with you, you can use a different workflow known as the
\href{https://www.atlassian.com/blog/git/git-branching-and-forking-in-the-enterprise-why-fork}{fork
and pull model}.

\begin{itemize}
\item
  \textbf{Fork} - This is the process of copying someones repository and
  creating a fork in the development branch where you can work the
  forked copy all you want without affecting the original code. When
  you're happy with the changes you made to the repo, you can make a
  pull request.
\item
  \textbf{Pull Requests} - I like to think of pull requests as an
  opportunity to review and assess external changes to the repo before
  you may decide to merge the fork back into the main branch. Something
  that helped clarify what a pull request is for me, was thinking about
  the difference between a push (write) request, and a pull (read)
  request. In the context of a shared repository, a pull request gives
  everyone that the repo is shared with a change to \emph{read} your
  request to consider the sum of your mini-milestones. Pull requests
  should amount to the completion of a component of your project. See
  \href{https://help.github.com/articles/about-pull-requests/}{here} for
  more info.
\item
  \textbf{Merge} - Once a Pull Request has been sufficiently reviewed,
  discussed, and tested (deployed), the new code can be merged with the
  master branch, typically by the team owner, or maintainer.
\item
  \textbf{Issues} - You can create an issue to raise a bug report,
  suggest some functionality, or discuss the objectives of a certain
  project.
\end{itemize}

\section{Best Practices}\label{best-practices}

\subsection{Citing analyses}\label{citing-analyses}

\begin{itemize}
\item
  Citing analyses can be done as text files that get included with data
  packages that can be distributed with manuscript.
\item
  Hakai has the ability to zip scripts and data files from a gitHUB repo
  and assign a DOI to it with a metadata entry. This isn't mutually
  exclusive to maintaining a GitHUB repo and pointing collaborators to
  archived versions of a repo.
\end{itemize}

\subsection{Private v. public repos}\label{private-v.-public-repos}

\begin{itemize}
\tightlist
\item
  We don't really want anonymous use of our analyses, so it's best to
  use private repos and add external collaborators if we want them to
  review or contribute to our analysis.
\end{itemize}

\subsection{Granularity of repos}\label{granularity-of-repos}

\begin{itemize}
\item
  Best practice is to not make repos too small.
\item
  In cases where you want to cite an analysis, need finer control of
  access permissions, or when a tool requires a separate repo (eg.
  creating an R package) you'll have to create a separate repo.
\item
  There are ways to take folders out of one repo to create an
  independent repo which retains commit history. Therefore, if you are
  uncertain about whether you need to split into a different repo, it's
  best to lump folders and split later if need be.
\item
  Managing repos with gitHUB `teams' is a good workflow.
\end{itemize}

\subsection{Code Licensing}\label{code-licensing}

\begin{itemize}
\item
  We recommend licensing with the
  \href{https://choosealicense.com/licenses/agpl-3.0/}{AGPLv3}
\item
  This license should be included in every script or data package, but
  we will still control who has access to code and data by requiring
  them to request access to data packages or scripts.
\end{itemize}

\chapter{Mapping in R}\label{mapping-in-r}

\section{Site Maps}\label{site-maps}

Section Contributors:

\begin{itemize}
\tightlist
\item
  Dr.~Daniel Okamoto
\item
  Dr.~Jenn Burt
\item
  Matt Whalen
\end{itemize}

A common task among most field researchers is the need to make a basic
site map to describe the location of your sampling. Using Arc GIS is the
most common way to produce maps at Hakai and requests can be made to the
GIS department, but sometimes a simple solution that could be
implemented in R is desired.

For a high resolution map with the resolution needed to see detailed
coastline features you can use the following code and shape file. To get
this to work on your computer, download the shape file and put it in a R
Studio project sub-folder called data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{message =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{warning =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{###################################################### #}
\NormalTok{###  Script to make a BC map                       }\AlertTok{###}\NormalTok{ #}
\NormalTok{###  Author:  D.K. Okamoto (modified by Jenn Burt) }\AlertTok{###}\NormalTok{ #}
\NormalTok{###################################################### #}

\CommentTok{# Libraries needed to run this code}
\KeywordTok{library}\NormalTok{(raster)}
\KeywordTok{library}\NormalTok{(maps) }
\KeywordTok{library}\NormalTok{(mapdata)}
\KeywordTok{library}\NormalTok{(maptools)}
\KeywordTok{library}\NormalTok{(rgeos)}
\KeywordTok{library}\NormalTok{(rgdal)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(ggsn)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(here)}
\end{Highlighting}
\end{Shaded}

\subsection{High Resolution Maps}\label{high-resolution-maps}

Download the folder \texttt{2\_Shapefile}
\href{https://drive.google.com/drive/folders/1GoEhACuGPGaQxn48ACkSc_qePx29TqC5?usp=sharing}{here}
and put it in a sub-directory of your working directory called
\texttt{data}.

This script assumes you are using the \texttt{here()} package in
conjunction with R-Studio projects to obviate setting your working
directory. See chapter 5 and the sub-section about the here() package to
read more.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{############## Make a map with sites ################## }
\NormalTok{########## Using high resolution shapefile ################}

\CommentTok{# To download the required shape file, first create a folder in your working directory called data}


\NormalTok{BC.shp <-}\StringTok{ }\KeywordTok{readOGR}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\StringTok{"2_Shapefile"}\NormalTok{, }\StringTok{"COAST_TEST2.shp"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## OGR data source with driver: ESRI Shapefile 
## Source: "/Users/brett.johnson/Documents/Projects/R-guide/data/2_Shapefile/COAST_TEST2.shp", layer: "COAST_TEST2"
## with 1 features
## It has 5 fields
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{### chose the lat/long extent you want to show}
\NormalTok{Ncalvert <-}\StringTok{ }\KeywordTok{extent}\NormalTok{(}\OperatorTok{-}\FloatTok{128.18}\NormalTok{, }\OperatorTok{-}\FloatTok{127.94}\NormalTok{, }\FloatTok{51.61}\NormalTok{, }\FloatTok{51.78}\NormalTok{)}

\NormalTok{### crop your shapefile polygons to the extent defined}
\CommentTok{# takes a moment to run (patience grasshopper)}
\NormalTok{BC.shp2 <-}\StringTok{ }\KeywordTok{crop}\NormalTok{(BC.shp,Ncalvert)}

\NormalTok{### project and fortify (i.e. turn into a dataframe)}
\NormalTok{BC.df <-}\StringTok{ }\KeywordTok{fortify}\NormalTok{(BC.shp2)}

\CommentTok{# (IF DESIRED) Load .csv file with your specific study site lat/longs}
\CommentTok{# this file is a dataframe with 4 columns: site_name, otterOcc(Y or N), lat, long  }
\CommentTok{# EXPTsites <- read.csv("/Users/jennb/Dropbox/Simple_BC_map/EXPTsites.csv", header = T)}

\CommentTok{# Jenn graph}
\CommentTok{# here is where you can see the styles of north arrow (scroll to bottom): http://oswaldosantos.github.io/ggsn/}
\CommentTok{# the high resolution shape file works well at this scale as it gives lots of the coastline detail}
\KeywordTok{ggplot}\NormalTok{()}\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data=}\NormalTok{ BC.df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long,}\DataTypeTok{y=}\NormalTok{lat,}\DataTypeTok{group=}\NormalTok{ group),}
      \DataTypeTok{colour=} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{fill=}\StringTok{'grey95'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{128.17}\NormalTok{, }\OperatorTok{-}\FloatTok{127.95}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{51.63}\NormalTok{, }\FloatTok{51.772}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_point(data=EXPTsites, aes(x=long, y=lat, shape=otter), size=4, colour="blue", stroke=1.5)+  #add this to plot site locations}
\StringTok{  }\CommentTok{#scale_shape_manual(values=c(21,24))+         #this makes different shapes for otter "yes" and otter "no" sites}
\StringTok{  }\KeywordTok{scalebar}\NormalTok{(BC.df, }\DataTypeTok{dist =} \DecValTok{3}\NormalTok{, }\DataTypeTok{st.size=}\DecValTok{4}\NormalTok{, }\DataTypeTok{height=}\FloatTok{0.01}\NormalTok{, }\DataTypeTok{dd2km =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{model =} \StringTok{'WGS84'}\NormalTok{, }\DataTypeTok{anchor =} \KeywordTok{c}\NormalTok{(}\DataTypeTok{x =} \OperatorTok{-}\FloatTok{127.96}\NormalTok{, }\DataTypeTok{y =} \FloatTok{51.63}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{north}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ BC.df, }\DataTypeTok{scale =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{symbol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{anchor=} \KeywordTok{c}\NormalTok{(}\DataTypeTok{x =} \OperatorTok{-}\FloatTok{128.15}\NormalTok{, }\DataTypeTok{y =} \FloatTok{51.775}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.minor =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{axis.title.y=} \KeywordTok{element_blank}\NormalTok{(), }\DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{axis.text.y=} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{), }\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{### if you want to make a larger Central coast map, just change the extent selected}
\NormalTok{CCoast <-}\StringTok{ }\KeywordTok{extent}\NormalTok{(}\OperatorTok{-}\FloatTok{128.48}\NormalTok{, }\OperatorTok{-}\FloatTok{127.9}\NormalTok{, }\FloatTok{51.5}\NormalTok{, }\FloatTok{52.1}\NormalTok{)}
\CommentTok{# crop the map to the new extent}
\NormalTok{CC.shp2 <-}\StringTok{ }\KeywordTok{crop}\NormalTok{(BC.shp,CCoast)}
\CommentTok{# fortify}
\NormalTok{CC.df <-}\StringTok{ }\KeywordTok{fortify}\NormalTok{(CC.shp2)}

\CommentTok{# Jenn graph}
\NormalTok{fig1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{()}\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data=}\NormalTok{ CC.df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long,}\DataTypeTok{y=}\NormalTok{lat,}\DataTypeTok{group=}\NormalTok{ group),}
      \DataTypeTok{colour=} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{fill=}\StringTok{'grey85'}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{128.5}\NormalTok{, }\OperatorTok{-}\FloatTok{127.95}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{51.63}\NormalTok{, }\FloatTok{52.05}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_point(data=EXPTsites, aes(x=long, y=lat, shape=otter), size=3.3, colour="blue", stroke=1.3)+  #add this to plot site locations}
\StringTok{  }\CommentTok{#scale_shape_manual(values=c(21,24))+       #this makes different shapes for otter "yes" and otter "no" sites}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{breaks=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{128.4}\NormalTok{, }\OperatorTok{-}\FloatTok{128.2}\NormalTok{, }\OperatorTok{-}\FloatTok{128.0}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{scalebar}\NormalTok{(CC.df, }\DataTypeTok{dist =} \DecValTok{5}\NormalTok{, }\DataTypeTok{st.size=}\FloatTok{3.5}\NormalTok{, }\DataTypeTok{height=}\FloatTok{0.014}\NormalTok{, }\DataTypeTok{dd2km =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{model =} \StringTok{'WGS84'}\NormalTok{, }\DataTypeTok{anchor =} \KeywordTok{c}\NormalTok{(}\DataTypeTok{x =} \OperatorTok{-}\FloatTok{128.33}\NormalTok{, }\DataTypeTok{y =} \FloatTok{51.64}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{north}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ CC.df, }\DataTypeTok{scale =} \FloatTok{0.07}\NormalTok{, }\DataTypeTok{symbol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{anchor=} \KeywordTok{c}\NormalTok{(}\DataTypeTok{x =} \OperatorTok{-}\FloatTok{128.465}\NormalTok{, }\DataTypeTok{y =} \FloatTok{52.056}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.minor =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{axis.title.y=} \KeywordTok{element_blank}\NormalTok{(), }\DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{axis.text.y=} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{), }\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{),}
          \DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{); fig1}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#I use this code to export a nice PDF file of specific dimensions.}
\KeywordTok{cairo_pdf}\NormalTok{(}\StringTok{"Fig1.pdf"}\NormalTok{, }\DataTypeTok{width=}\DecValTok{4}\NormalTok{, }\DataTypeTok{height=}\DecValTok{5}\NormalTok{)}
\KeywordTok{print}\NormalTok{(fig1)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## pdf 
##   2
\end{verbatim}

\subsection{Medium Resolution PBS Mapping
Package}\label{medium-resolution-pbs-mapping-package}

The Pacific Biological Station in Nanaimo has put together a mapping
package that contains some medium resolution files of the Pacific Coast.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{############## Make a map with the sites ################## #}
\NormalTok{############# Using DFO coastline data file ################}

\CommentTok{#this is lower resolution than the shapefile above}

\KeywordTok{library}\NormalTok{(PBSmapping)}

\NormalTok{## Plot the map}
\KeywordTok{data}\NormalTok{(nepacLLhigh)       }\CommentTok{# DFO BC Coastline data - high resolution}
\KeywordTok{plotMap}\NormalTok{(nepacLLhigh, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{128.52}\NormalTok{, }\OperatorTok{-}\FloatTok{127.93}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{51.56}\NormalTok{, }\FloatTok{52.07}\NormalTok{), }\DataTypeTok{col=}\StringTok{"grey90"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{tckMinor =} \DecValTok{0}\NormalTok{,}
        \DataTypeTok{xlab=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{, }\DataTypeTok{lwd=}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}

\CommentTok{#add a scale bar}
\KeywordTok{map.scale}\NormalTok{(}\DataTypeTok{x=}\OperatorTok{-}\FloatTok{128.455}\NormalTok{, }\DataTypeTok{y=}\FloatTok{51.61}\NormalTok{, }\DataTypeTok{ratio=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{relwidth=}\FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }
\CommentTok{# # add site points}
\CommentTok{# points(EXPTsites$long, EXPTsites$lat, cex=1.5, pch=20, size=2)}
\end{Highlighting}
\end{Shaded}

\subsection{Low Resolution Pacific Coast
Maps}\label{low-resolution-pacific-coast-maps}

For lower resolution maps to represent larger scales you can use the
maps from the \texttt{maps} and \texttt{mapdata} packages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{############## Pacific Coast Map ##################}
\NormalTok{################################################### #}

\KeywordTok{library}\NormalTok{(maps)}
\KeywordTok{library}\NormalTok{(mapdata)}

\CommentTok{#creata a data file to make a basemap}
\CommentTok{# this database has a lower resolution (which is fine for large scale map)}
\NormalTok{m <-}\StringTok{ }\KeywordTok{map_data}\NormalTok{(}\StringTok{"world"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"usa"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{))}

\CommentTok{#this database has a way higher resolution}
\NormalTok{d <-}\StringTok{ }\KeywordTok{map_data}\NormalTok{(}\StringTok{"worldHires"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"usa"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{))}

\CommentTok{#make a basic map, all one colour}
\CommentTok{# play around with xlim and ylim to change the extent}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group)) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\StringTok{"conic"}\NormalTok{, }\DataTypeTok{lat0 =} \DecValTok{18}\NormalTok{, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{210}\NormalTok{, }\DecValTok{237}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{46}\NormalTok{,}\DecValTok{62}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make different colours for Alaska and USA and Canada}
\NormalTok{## I have tried to figure out how to get the Provinces borders to show, to no avail... if someone else has code for this, share!}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(m, region}\OperatorTok{==}\StringTok{"Canada"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{fill=}\StringTok{"grey65"}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\NormalTok{.}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(m, region}\OperatorTok{==}\StringTok{"USA"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{fill=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\NormalTok{.}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_polygon(data = subset(d, region=="Mexico"), aes(x=long, y = lat, group = group), fill="white", colour="black", size=.1) +}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\StringTok{"conic"}\NormalTok{, }\DataTypeTok{lat0 =} \DecValTok{18}\NormalTok{, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{195}\NormalTok{, }\DecValTok{238}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\FloatTok{62.5}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"grey"}\NormalTok{),  }\CommentTok{#change "grey" to NA to remove}
          \DataTypeTok{axis.title=} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{axis.text=} \KeywordTok{element_blank}\NormalTok{(),     }
          \DataTypeTok{axis.ticks =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# playing with extent and colour}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(d, region}\OperatorTok{==}\StringTok{"Canada"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(d, region}\OperatorTok{==}\StringTok{"USA"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{fill=}\StringTok{"darkgrey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{195}\NormalTok{, }\DecValTok{240}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{45}\NormalTok{,}\DecValTok{65}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# here you can see that if you use the other dataframe "d" the resolution is much higher. }
\CommentTok{# this is good for smaller chunks of the BC coast, but less good for a PNW map}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(d, region}\OperatorTok{==}\StringTok{"Canada"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{fill=}\StringTok{"darkgrey"}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\NormalTok{.}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(d, region}\OperatorTok{==}\StringTok{"USA"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y =}\NormalTok{ lat, }\DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{fill=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{colour=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{size=}\NormalTok{.}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_polygon(data = subset(d, region=="Mexico"), aes(x=long, y = lat, group = group), fill="white", colour="black", size=.1) +}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\StringTok{"conic"}\NormalTok{, }\DataTypeTok{lat0 =} \DecValTok{18}\NormalTok{, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{195}\NormalTok{, }\DecValTok{240}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{45}\NormalTok{,}\DecValTok{61}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.minor =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{colour =} \OtherTok{NA}\NormalTok{),}
          \DataTypeTok{axis.title=} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{axis.text=} \KeywordTok{element_blank}\NormalTok{(),}
          \DataTypeTok{axis.ticks =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-9-1.pdf}

\section{Interactive Maps with
Leaflet}\label{interactive-maps-with-leaflet}

It is possible to use the JavaScript based widget known as
\href{https://rstudio.github.io/leaflet/}{Leaflet} from within R which
is great for making interactive maps in an interactive R Markdown
document, as well as in Shiny apps. You can use base maps that Hakai
hosts from the ArcGIS Online Server (AGOL), or you can use the basic
maps that are provided. You are able to add points, visualize heat maps,
legends, cloropleths, and other simple GIS tasks.

To do this from R you need to install these packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The package that allows you add leaflet map widgets to your interactive web documents:}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{'leaflet'}\NormalTok{)}

\CommentTok{# The package that allows you to use map tiles from Hakai's AGOL}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{'leaflet.esri'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaflet)}
\KeywordTok{library}\NormalTok{(leaflet.esri)}
\KeywordTok{library}\NormalTok{(magrittr)}
 \KeywordTok{leaflet}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\CommentTok{# Set the default lat and long and zoom level}
\StringTok{        }\KeywordTok{setView}\NormalTok{(}\DataTypeTok{lng =} \OperatorTok{-}\DecValTok{126}\NormalTok{, }\DataTypeTok{lat =} \FloatTok{50.3601}\NormalTok{, }\DataTypeTok{zoom =} \DecValTok{7}\NormalTok{) }\OperatorTok{%>%}
\StringTok{        }\CommentTok{# Add default tile}
\StringTok{        }\KeywordTok{addTiles}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\CommentTok{# Add tile from Hakai AGOL}
\StringTok{        }\KeywordTok{addEsriTiledMapLayer}\NormalTok{(}\DataTypeTok{url =} \StringTok{"https://ags.hakai.org:6443/arcgis/rest/services/AGOL_basemaps/Marine_charts/MapServer"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{htmlwidget-22474e4b02ff188e2adc}{}

Follow the instructions on R-Studio's
\href{https://rstudio.github.io/leaflet/}{Leaflet package webpage} to
create your map. The included base maps can be viewed
\href{https://leaflet-extras.github.io/leaflet-providers/preview/index.html}{here}

To use the base maps on Hakai's AGOL server use the
\texttt{addEsriTiledMapLayer()} function from the \texttt{esri.leaflet}
package where the documentation can be read
\href{https://bhaskarvk.github.io/leaflet.esri/reference/addEsriTiledMapLayer.html}{here}

Hakai ArcGIS Online maps can be viewed
\href{https://ags.hakai.org:6443/arcgis/rest/services/AGOL_basemaps/}{here}.
You can first view any of these base maps in AGOL to decide which you'd
like to use. Once you've decided, you simply copy the URL and paste it
into the \texttt{addEsriTileMapLayer()} function

\section{Working with other file
types}\label{working-with-other-file-types}

You can work with almost any georeferenced file in R. Here's an example
in which use google drive to access a GeoTIFF of Calvert Island
generated by the Hakai Geospatial Team, then plot locations of Hakai's
Autonomous Reef Monitoring Systems (\url{http://www.oceanarms.org/})

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Author: Matt Whalen}

\CommentTok{# load additional libraries}
\KeywordTok{library}\NormalTok{( googledrive ) }\CommentTok{# to access GeoTIFF}
\KeywordTok{library}\NormalTok{( scales ) }\CommentTok{# for transparent colors}

\CommentTok{# read the GeoTIFF using package googledrive}
\CommentTok{# note: you need to be able to log in to google drive to use this method}
\NormalTok{temp <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{( }\DataTypeTok{fileext =} \StringTok{".tif"}\NormalTok{ )}
\KeywordTok{drive_download}\NormalTok{( }
  \KeywordTok{as_id}\NormalTok{(}\StringTok{"1hj5KpiRGcPF11pM-oMd13LRCL1uG0pCm"}\NormalTok{), }\DataTypeTok{path =}\NormalTok{ temp, }\DataTypeTok{overwrite =} \OtherTok{TRUE}\NormalTok{ )}

\CommentTok{# use raster package to load the file into R}
\NormalTok{ras <-}\StringTok{ }\KeywordTok{brick}\NormalTok{( temp )}
\CommentTok{# warning message suggests the file has "rotation". Can override this}
\NormalTok{ras}\OperatorTok{@}\NormalTok{rotated <-}\StringTok{ }\OtherTok{FALSE}

\CommentTok{# Either read in a .csv file of your sites with lat and long, or create the dataframe manually as done here.}
\NormalTok{xy <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{c}\NormalTok{( }\OperatorTok{-}\FloatTok{128.0946}\NormalTok{,}\OperatorTok{-}\FloatTok{128.0739}\NormalTok{,}\OperatorTok{-}\FloatTok{128.0714}\NormalTok{,}\OperatorTok{-}\FloatTok{128.2643}\NormalTok{,}\OperatorTok{-}\FloatTok{128.2370}\NormalTok{,}\OperatorTok{-}\FloatTok{128.2542}\NormalTok{,}\OperatorTok{-}\FloatTok{128.0091}\NormalTok{,}
         \OperatorTok{-}\FloatTok{128.1512}\NormalTok{,}\OperatorTok{-}\FloatTok{127.9967}\NormalTok{,}\OperatorTok{-}\FloatTok{128.1552}\NormalTok{,}\OperatorTok{-}\FloatTok{128.1261}\NormalTok{,}\OperatorTok{-}\FloatTok{128.1399}\NormalTok{ ),}
  \DataTypeTok{y =} \KeywordTok{c}\NormalTok{( }\FloatTok{51.66462}\NormalTok{,}\FloatTok{51.70118}\NormalTok{,}\FloatTok{51.74418}\NormalTok{,}\FloatTok{51.84152}\NormalTok{,}\FloatTok{51.81435}\NormalTok{,}\FloatTok{51.80363}\NormalTok{,}\FloatTok{51.73073}\NormalTok{,}\FloatTok{51.64908}\NormalTok{,}
        \FloatTok{51.73390}\NormalTok{,}\FloatTok{51.64910}\NormalTok{,}\FloatTok{51.68025}\NormalTok{,}\FloatTok{51.66703}\NormalTok{),}
  \DataTypeTok{habitat =} \KeywordTok{c}\NormalTok{( }\StringTok{"Macro"}\NormalTok{,}\StringTok{"Barren"}\NormalTok{,}\StringTok{"Barren"}\NormalTok{,}\StringTok{"Nereo"}\NormalTok{,}\StringTok{"Macro"}\NormalTok{,}\StringTok{"Macro"}\NormalTok{,}\StringTok{"Barren"}\NormalTok{,}
               \StringTok{"Macro"}\NormalTok{,}\StringTok{"Barren"}\NormalTok{,}\StringTok{"Nereo"}\NormalTok{,}\StringTok{"Nereo"}\NormalTok{,}\StringTok{"Nereo"}\NormalTok{ )}
\NormalTok{)}

\CommentTok{# project coordinates for sites to match geotiff}
\CommentTok{# identify latitude and longitude to make SpatialPointsDataFrame}
\KeywordTok{coordinates}\NormalTok{( xy ) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{)}
\CommentTok{# give it some kind of coordinate reference system}
\KeywordTok{proj4string}\NormalTok{( xy ) <-}\StringTok{ }\KeywordTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +datum=WGS84"}\NormalTok{)  }
\CommentTok{# reporject to the CRS of the GeoTIFF raster}
\NormalTok{res <-}\StringTok{ }\KeywordTok{spTransform}\NormalTok{( xy, ras}\OperatorTok{@}\NormalTok{crs )}

\CommentTok{# crop the map to focus on the area that contains all sites}
\NormalTok{rasc <-}\StringTok{ }\KeywordTok{crop}\NormalTok{( ras, }\KeywordTok{extent}\NormalTok{(res)}\OperatorTok{*}\FloatTok{1.5}\NormalTok{ ) }\CommentTok{# multiplying by 1.5 gives a buffer}

\CommentTok{# plot the cropped raster}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{plotRGB}\NormalTok{( rasc, }\DataTypeTok{maxpixels=}\DecValTok{5000000}\NormalTok{ )}

\CommentTok{# add points with custom color scheme}
\NormalTok{cols <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"gray25"}\NormalTok{,}\StringTok{"#5ab4ac"}\NormalTok{,}\StringTok{"#d8b365"}\NormalTok{)}
\KeywordTok{points}\NormalTok{( res, }\DataTypeTok{cex=}\DecValTok{2}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{21}\NormalTok{, }
        \DataTypeTok{bg=}\NormalTok{scales}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(cols[}\KeywordTok{as.numeric}\NormalTok{(xy}\OperatorTok{$}\NormalTok{habitat)],}\FloatTok{0.7}\NormalTok{) )  }
\end{Highlighting}
\end{Shaded}

\includegraphics{Hakai-Institute-R-Stats-Guide-for-Reproducible-Analyses_files/figure-latex/unnamed-chunk-12-1.pdf}

\chapter{Resources and References}\label{resources-and-references}

\textbf{The one and only introductory R book to read from front to
back:}

\begin{itemize}
\tightlist
\item
  \href{http://r4ds.had.co.nz/}{R For Data Science}
\end{itemize}

\textbf{Workflows for Scientific Computing}

\begin{itemize}
\tightlist
\item
  \href{http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510}{Good
  Enough Practices for Scientific Computing}
\item
  \href{http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745}{Best
  Practices for Scientific Computing}
\end{itemize}

\textbf{Functional Programming} (Loops and stuff)

\begin{itemize}
\tightlist
\item
  \href{http://ctlente.com/en/purrr-magic/}{Purr Tutorial}
\item
  \href{\%22http://adv-r.had.co.nz/\%22}{Advanced R}
\end{itemize}

\textbf{Git and GitHub}

\begin{itemize}
\tightlist
\item
  \href{http://happygitwithr.com/}{Happy Git with R} by Jenny Bryan
\end{itemize}

\textbf{Plots}

\begin{itemize}
\tightlist
\item
  \href{http://www.cookbook-r.com/Graphs/}{Cookbook for R}
\end{itemize}

\textbf{Statistical Modeling Examples}

\begin{itemize}
\tightlist
\item
  \href{https://stats.idre.ucla.edu/}{UCLA Institute for Digital
  Research and Education}
\item
  \href{https://github.com/seananderson/glmm-course}{Generalized Linear
  Models Tutorial by Sean Anderson}
\end{itemize}

\textbf{Statistics}

\begin{itemize}
\tightlist
\item
  \href{https://sunsetridgemsbiology.wikispaces.com/file/view/Choosing+and+Using+Statistics.pdf}{Undergraduate
  Biologist's Guide with R Examples}
\end{itemize}

\textbf{Tidyverse}

\begin{itemize}
\tightlist
\item
  \href{http://www.tidyverse.org/}{Tidyverse Website}
\item
  \href{http://style.tidyverse.org/}{Tidyverse style guide}
\end{itemize}

\chapter{Glossary of Terms}\label{glossary-of-terms}

\textbf{Branching}: From GitHUB: ``A branch is a parallel version of a
repository. It is contained within the repository, but does not affect
the primary or master branch allowing you to work freely without
disrupting the''live" version. When you've made the changes you want to
make, you can merge your branch back into the master branch to publish
your changes."

\textbf{Commit}: From GitHub: ``A commit, or''revision``, is an
individual change to a file (or set of files). It's like when you save a
file, except with Git, every time you save it creates a unique ID
(a.k.a. the''SHA" or ``hash'') that allows you to keep record of what
changes were made when and by who. Commits usually contain a commit
message which is a brief description of what changes were made."

\textbf{Developing Analyses}: A term coined by Hillary Parker that
describes the process by which a data anlaysis is ensured to be
reproducible, accurate and collaborative.

\textbf{Distributed Version Control}: A system of tracking the changes
in a set of files that allows devlopers to work on locally stored copies
of the same set of files, before sharing the changes with a remote copy
of the set of files.

\textbf{Fork}:From GitHub: ``A fork is a personal copy of another user's
repository that lives on your account. Forks allow you to freely make
changes to a project without affecting the original. Forks remain
attached to the original, allowing you to submit a pull request to the
original's author to update with your changes. You can also keep your
fork up to date by pulling in updates from the original.''

\textbf{Issues}: From GitHub: ``Issues are suggested improvements, tasks
or questions related to the repository. Issues can be created by anyone
(for public repositories), and are moderated by repository
collaborators. Each issue contains its own discussion forum, can be
labeled and assigned to a user.''

\textbf{Local}: The set of your repositories files that are stored on
your own computer.

\textbf{Literate Programming}: A programming paradigm invented by Donals
Knuth that puts the emphasis on human readability of code and flow of
code structure that is logical to the human.

\textbf{Merge}: From GitHub: ``Merging takes the changes from one branch
(in the same repository or from a fork), and applies them into another.
This often happens as a pull request (which can be thought of as a
request to merge), or via the command line. A merge can be done
automatically via a pull request via the GitHub web interface if there
are no conflicting changes, or can always be done via the command line.
For more information, see''Merging a pull request."

\textbf{Remote}: From GitHub: ``This is the version of something that is
hosted on a server, most likely GitHub. It can be connected to local
clones so that changes can be synced.''

\textbf{Pull Request}: From GitHub: ``Pull requests are proposed changes
to a repository submitted by a user and accepted or rejected by a
repository's collaborators. Like issues, pull requests each have their
own discussion forum.''

\textbf{Push}: From GitHub: ``Pushing refers to sending your committed
changes to a remote repository, such as a repository hosted on GitHub.
For instance, if you change something locally, you'd want to then push
those changes so that others may access them.''

\textbf{Working Directory}: The folder in which your R session is
reading and writing files from and to.


\end{document}
