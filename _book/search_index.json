[
["index.html", "Hakai Institute R Data-Analyst Guide Preface", " Hakai Institute R Data-Analyst Guide Brett Johnson Latest update: 2017-07-28 Preface This is the Hakai Institute’s Data Analyst Guide. One of Hakai’s core themes that cross-cuts every research axis is ‘Big-Data and Modelling’. Our ability to gain insight from our long-term ecological data sets is limited only by our capacity to integrate, and digest data. To that end, this guide was developed to serve as a foundation from which to build the internal technical capacity — using a coherent and systematic approach — to turn data into insight, understanding, and ultimately knowledge about the ecosystems we study. This book will teach you how to develop analyses in the R programming language. You’ll learn project workflow and organization, literate programming style, reproducible research techniques, and collaborative analysis development in an open-science context. The Hakai Institute is a research organization based on the coast of British Columbia, Canada. See the Hakai Institute Website for more information. "],
["introduction.html", "1 Introduction 1.1 University Gaps 1.2 Reproducibility Problem 1.3 Modern Approach to Data Analysis 1.4 Acknowledgements", " 1 Introduction 1.1 University Gaps Most University courses that use R are focused on teaching statistical techniques such as generalized linear models, or logistic regression — which is very important — but pay no attention to educating students on how to actually conduct an analysis from start to finish. Questions that often remain un-answered include: What is an efficient workflow? How do I get data? How do I clean and manipulate my data into a format to analyze? How do I make my analysis reproducible in case I get new data or someone else wants to run my code? How can I collaborate on this analysis? How can I get my analysis into a format for someone to meaningfully conduct peer-review? How do I maintain version-control of my analysis? How do I efficiently produce a professional report or other artifact of my analysis to distribute? This guide aims to bridge this gap by taking you through an example of a well-developed analysis that will form a template for conducting your own analyses while working with Hakai. You will be directed through an efficient way to learn R, and progress to modern methods of communicating results. Learning to develop analyses in R can be a very frustrating and difficult experience. Know that we have all suffered severely in this way, but that this guide will hasten you through the drudgery and frustration. Combined with a graduate level understanding of statistics, this guide should get you to an intermediate skill level in R-Studio using a very high-level, and modern approach. 1.2 Reproducibility Problem An emerging concern with modern scientific studies is the inability to reproduce study findings. As experimental and analytic methods become increasingly complex, published methods often lack the detail required to be able to reproduce the study. As data analysts, it is our responsibility to include in our analyses sufficient notation of the analysis to facilitate deep understanding of what was done to import, tidy, transform, visualize, and model your data. Some academic journals, such as the Journal of Bio-statistics, are taking proactive steps by adopting a reproducibility policy for article submissions. The journal of Nature has taken a stance on reproduce-ability and has published several articles in a special called Challenges in Irreproducible Research. For some comic relief on the follies of working with someone who doesn’t use reproducible research methods watch this youtube video 1.3 Modern Approach to Data Analysis There is a vibrant ‘open-source’ community of people developing methods, packages, and workflows in the R programming world. Consequently, we have some of the most modern, flexible, and high-level methods to develop and communicate statistical analyses. However, as the R community iterates ideas of how the programming language works at a very base level over the last couple of decades, we are left with a litany of methods and programming syntax’s. There are typically at least half a dozen ways to write a chunk of code to reach the same desired result in R. This, in part, has given R the reputation as being difficult to learn. This guide aims to address the ‘thousand and one way of doing things’ problem in R by focusing on the recent development of packages that form a simple, elegant, and coherent grammar of data analysis. This collection of packages and methods is known as the ‘tidy-verse’, developed in large part by the Chief Scientist of R-Studio, Hadley Wickham, and has recently gained traction within enterprise and academic applications of the R language. The objectives of this book are to serve as a: Guide to install, set-up, and become familiar with the data analysts tools; R, R-Studio, Git and Git Hub. Best-practice guidelines to develop reproducible, accurate, and collaborative analyses in R-Studio. 1.4 Acknowledgements Much of this document refers you to material that others have worked very hard to make. I simply point to these resources in an order that makes sense and seems systematic to me. Many thanks to the following people for making this possible: Dr. Jenny Bryan, Statistics Professor at UBC, RStudio Twitter, GitHUB. Dr. Roger Peng, Professor of Bio-statistics at Johns Hopkins University. Website, GitHUB. Dr. Hadley Wickham, Chief Scientists at R-Studio. Website, Twitter, GitHUB. Dr. Hillary Parker, Stitch-fix "],
["a-good-data-analysis.html", "2 A Good Data Analysis 2.1 Reproducible Research 2.2 Literate Programming 2.3 Open Science Collaboration 2.4 Resources", " 2 A Good Data Analysis Defining a perfect data analysis is nearly impossible. However, new ideas about what makes a good data analysis are emerging. With data being so readily available in vast quantities, analyzing data using out of date methods — such as Microsoft excel — quickly becomes overwhelming, not reproducible, error-prone, and difficult to assess for reliability. Much of the progress in terms of ‘developing analyses’ has been made in the field of bio-statistics due to the high volume of genomic data that researchers deal with. One of the most concerning examples of what can go wrong with an analysis, is from the field of genomics and cancer treatments. In the ‘Duke Scandal’, researchers made mistakes in their data analysis, that were extremely difficult to track, and resulted in patients receiving the wrong cancer treatment. This is an extreme example that affected peoples lives directly. I would argue, that the work that we do at Hakai, analyzing ecological data, has much broader implications and should be treated with an equal degree of discretion. Some important concepts in defining a good data analysis are: Reproducible Research, Literate Programming, and The Open Notebook These three concepts bring together a very modern way of conducting science that are beneficial in the following regards: They save you work in the long run by allowing you to reproduce your own analyses after you’ve long forgotten the details of how they were conducted If you weave a narrative text into your computer code you’ll be able to understand what you were thinking at a later time when you revisit it You can easily collaborate You can show your peers that you have nothing to hide in your analytic methods You can share your analyses in hopes that others will improve the quality of your analysis by offering their insight 2.1 Reproducible Research If your study finds something very interesting, people are going to want to know how you came to your conclusion. A simple example of how easy it is to break the reproducibility chain comes from working with your data in excel. By simply deleting some values that looked to be outliers, without recording anywhere that you did that, or why you did that, you have effectively broken the reproducibility chain. Another person could not come to the same conclusions as you did, if you provided them the raw data set you started with. In order for your analysis to be viewed as trustworthy, you need to be able to provide the data, the scripted code you used to clean, summarize, model, and visualize that data, and then a reviewer has to be able to run that same code and see the same results. This level of transparency allows a reviewer to look very closely at how you conducted your analysis. This adds an additional step in the peer review process which has not previously been possible with un-scripted analyses. The reality is, the peer-reviewer or collaborator that you will most often want to work with is ‘future-you’. Scripting reproducible analyses with embedded narrative allows future-you to understand what past-you was thinking. This, in turn, saves you a lot of time in the long run, and allows you to build upon your previous work. The idea of embedding your own narrative, or adding comments to your code, introduces the idea of literate programming. 2.2 Literate Programming By weaving together human-readable narrative that explains what your computer code is doing and why you decided to do it, greatly increase the quality of your work. R Markdown visual example. (code to html report) 2.3 Open Science Collaboration 2.3.1 Peer Review Open science collaboration establishes an additional peer-review step in the scientific process. By making your analysis public using a distributed version control system you open up the possibility for on-going peer-review of your analysis, as well as the the opportunity for not only your immediate colleagues, but also experts in your field to contribute in a meaningful and formalized way. 2.3.2 Code Review Before you publish or distribute your analysis, you must have a colleague reveiw your code. You would never publish a written article without sending it to a trusted co-worker or superviser first. Code review is the same thing. While peer-review permits the scientific merit of your analyses to be assesed, code-review permits a colleague to assess your analyses for coding errors. Developing analyses is difficult and error prone. Using the distributed version control system GitHUB, you are able to track changes to your code over time and enable others to suggest edits for your review, leaving you with a history of exactly what was added when and by whom — this is version control. The distributed aspect of ‘distributed version control’ means that multiple people can access the version control through your repository of all the files of your analysis (and their previous versions). You can make your repository (repo) private or public. By acessing your repo, which is hosted on Git HUB, others can meaningfully collaborate, conduct peer-review, and code-review. 2.3.3 Collaboration Software developers have been using this type of development platform for some time, and by using these tools data analysis can progress towards the concept of developing analyses. Ultimately, this approach makes your life easier. If you’ve experienced the pain of digging through your emails or zip files to find the analysis you or your colleague conducted years ago, only to be left perplexed by which version is the ‘master’ or ‘final’ then Git HUB will be a welcome addition to your toolbox. Git Hub can also be used as a project management tool for example by creating to-do lists for yourself and your team. By creating issues you can leave yourself a to-do list, or collaborators can identify issues with your analysis. We will cover Git HUB fundamentals in the next chapter. 2.4 Resources The ‘bible for a new generation of Data Scientists’ is Hadley Wickham and Garrett Grolemund’s Book: R For Data Science. This book presents a modern approach to data analysis and leads you to master the ‘tidy-verse’; a combination of R packages and a well thought out and systematic approach to “import, tidy, transform, visualize, and model data.” Using the tidy-verse as a foundation for your coding replaces the ‘thousand and one ways’ of doing things in R into a modern and concise grammar of data analysis development. "],
["data-analysts-toolbox.html", "3 Data Analyst’s Toolbox 3.1 R and R Studio 3.2 Version control with Git and GitHUB", " 3 Data Analyst’s Toolbox 3.1 R and R Studio R is the statistical programming language that R Studio uses to communicate. I think of R Studio as being the front-end user interface to the coding language R which works in the background. For this all to work you you need to download two things to start; R and R Studio. Install R: Go to https://cran.r-project.org/ and follow the link to the download for your operating system. Install R Studio: Go to https://www.rstudio.com/products/rstudio/download/ to download R studio Now that you have downloaded the tools you need, lets get familiar first with R-Studio and then the R programming language. 3.1.1 Step-by-step R/RStudio R Studio Familiarization: Watch this R-Studio Tutorial for a nice introduction to the software. Start Plotting: Now that you’re familiar with the software, lets get your feet wet in R and R-Studio. I recommend you start making some plots by working through the following exercises: Section 3.1 to 3.4 in Hadley Wickham’s R for Data Science Book. Projects Workflow: Watch R-Studio tutorial: Projects and R Studio Install Hakai’s Application Programming Interface This API is installed directly in R-Studio and allows you to download data from the Hakai Database. Open up R Studio and in the console window copy and paste the following code: install.packages(&#39;devtools&#39;) install.packages(&#39;tidyverse&#39;) library(&#39;devtools&#39;) library(&#39;tidyverse&#39;) devtools::install_github(&quot;HakaiInstitute/hakai-api-client-r&quot;, subdir=&#39;hakaiApi&#39;) 3.2 Version control with Git and GitHUB Version control is an additional level of saving your files. While writing your R scripts in R Studio you should save your files to your desktop like you normally would in a word file for example, by clicking save or the floppy disk button. Use the same filename, so that each time you save the previous verison is over-written. No need to have multiple versions of the same file on your computer with different dates or initials to identify the version you want to work on (eg. fishy_analysis_V9_2017_05_11_BJ_Bad_Scoobies.R, etc…). We’ve all done that. But it’s annoying to keep track of and remember what version is the one you want, and it’s difficult to create meaningful versions of the file that you want that represent some sort of miletstone, especially when everything is floating around in email land. The additional ‘save’ that comes from version control is known as a commit. You save your files like you normally would in word, but every once in a while you commit your files as a version to be remembered. A commit can be thought of as a bullet point in the to do list of your analysis, and each commit you make must be accompanied by a message. For example; ‘read in data and tidy it up’, or ‘remove observations from non-standard sampling event, and re-fit GLM’. Git tracks the commits you make in R-Studio locally on your own computer. When you are ready for a series of commits to be made public, you push your commits to your remote repository at Git Hub. Similar to the relationship between R and R-Studio, Git is the local version control system running in the background on your computer while Git Hub is the remote user interface for saving, tracking, and sharing updated versions of your analyses that are hosted on a remote server. Git and Git HUB are integral for tracking the evolution of a set of files, and ultimately the development of your analysis. If you’re interested in learning why version control is important, I encourage you to skim Chapter 1; ‘Why Git? Why GitHUB’ from Jenny Bryan’s Book. I recommend referring back to this book whenever you have a question about using git and Git Hub with R-Studio. You can also watch this video for an introduction to Git and Git Hub. 3.2.1 Step-by-step Git/Git HUB Setup a Git HUB Account: First, you’re going to want to sign up for a GitHUB account Issues: (Need to make this repo public for this to be possible) Now that you have a Git Hub account, you should introduce yourself and let us know you’re going to work through this material. To do that go to the issues tab in this repo and ‘create a new issue’ and type a message to introduce yourself. Tell us what project you are working on, and what your first data analysis project will likely be. Using issues is also a great way to ask questions about course material, or suggest improvements to the guide. Remember, during a normal data analysis, the issues function on Git HUB can be used as a way for reviewers to identify issuyes with your analysis or code, or simply as a to-do list for yourself or your team. Fork this repo: Next, you should fork this repo. Think of forking a repo as creating a divergent copy of a set of files — like a fork in the road, two developing versions of the same original set of files can develop. You can edit your fork without comprimising the master version. Normally when you are happy with your changes and you want to suggest your edits to the owner you make A pull request. A pull request is simply your request to the owner of the master copy to pull your edits into the master version. This is a simple explanation of how collaborative editing works with Git HUB. To successfully fork this repo and suggest edits simply go to my repo here and click on the name of this chapter: ‘new_analysts.Rmd’, once it’s open click on the pencil icon to suggest edits. Editing in this manner automatically forks the repo and makes a copy in your own list of repos where you will be able to see all the files that are available for others to view and edit. If you find an error anywher in this guide, edit it, and make a ‘pull request’ to suggest edits to me. This serves two purposes: i) the quality of this guide will be improved if you take time to edit chapters and contribute your insights on what didn’t work for you; and ii) forking repos and making pull-requests is training you for the workflow you will use when contributing to colleagues’ analyses in the very near future. Now you can make all the edits you’d like and when your happy you can click on ‘pull request’ to send the updates back to me so I can review them and decide if I should accept your edits and merge them into the master version. If you aren’t successful forking my repo, read Chapter 24 of Jenny Bryan’s Book; ‘Fork a Repo’. Install and set-up Git: Installing Git locally and getting Git to communicate to your remote Git Hub website account, and then getting them to talk with R Studio takes a number of steps to complete…This next part can be painful to work through, but it is 100 % necessary and well worth it in the long-run. Install Git and Git Hub and get them talking to R-Studio using Jenny Bryan’s Guide, start at chapter 7 and work to the end of chapter 13. Put this all in context: To learn how to use Git, Git Hub and R-Studio I recommend watching the R-Studio Essentials tutorial; GitHUB and R-Studio. Congratulations you now have setup the three main tools in the R Analyst toolbox: R, R Studio and Git Hub and learned the basics of each component. Now that you’ve seen almost all the components of a good workflow, in the next chapter you can put all these things together and work through an example of a data analysis using a prescribed, modern approach. "],
["developing-analyses-with-the-hakai-institute.html", "4 Developing Analyses with the Hakai Institute 4.1 General Principes 4.2 Step-by-step Analysis Example 4.3 Programming style guide 4.4 Git flow 4.5 Collaboration with phyton users", " 4 Developing Analyses with the Hakai Institute 4.1 General Principes Packrat, roxygen 4.2 Step-by-step Analysis Example In this section I will walk you through an example of a project that brings together all the different concepts we’ve been talking about up to this point. This project should serve as an example of how to develop analyses when working with the Hakai Institute. I start every new project in the following manner. I first create a Git HUB repo, start a new R-Studio project and link it to my Git HUB repo, and then I populate my working directory with a template folder structure that stores the different categories of files I produce from every analysis in Google Drive. Make sure you have the Google Drive desktop app installed on your computer before you proceed, so you can save files on your desktop that will automatically sync with Google Drive. 4.2.1 Set up a repo on GitHUB Go to https://github.com and login. Create a new repository on your own account, or on the Hakai account depending on the scenario. Call it something-descriptive-but-concise, for example. Make it public and click yes to initialize with a README. Click the big green button “Create repository.” Copy the HTTPS clone URL to your clipboard via the green “Clone or Download” button. You’ll need this to link the repo with R-Studio. 4.2.2 Start a new R-Studio project Open R Studio and go to File &gt; New Project &gt; Version Control &gt; Git. In the “repository URL” paste the URL of the Git Hub repository you just made. Create the project in your programs Google Drive folder for analyses and create a new folder with a descriptive title for your analysis. Just make sure. Click “Create Project” This should download the README.md file that we created on GitHub in the previous step. Look in RStudio’s file browser pane for the README.md file.&quot; 4.2.3 Create folder structure I use a default folder structure for every analysis based on the files that are produced from every analysis. Using the directory that you created your new R-Studio project, which will be your working directory, create these sub-folders within the project folder: raw data processed data raw scripts final scripts exploratory figures final figures Using Google Drive to store files has advantages, but you must be careful not to edit files directly in Google Drive without committing changes using Git. Doing that risks putting your repo out of sync with your local version. So only ever make changes file stored in your Google Drive via R, and commit changes to them using Git. 4.2.4 Pre-process data: import &amp; tidy I usually create a few different scripts in any analysis, which helps me to compartmentalize the different steps of the analysis. I start with a pre-processing script that will read in and format all the different data sets I want to combine and analyze. This is where I make changes to data format that I want to apply to all subsequent uses of that data (so that I don’t have to keep doing it in subsequent scripts). Create pre_process.R script In your newly created R Studio project, go to File &gt; New File &gt; R Script. Save it in the raw scripts sub-directory of your project directory. Set your working directory Load packages Import data Method 1:From spreadsheet you already have Most often you’re going to want to read in files that are .csv files. These are comma separated value files and can be produced from excel or Google sheets by saving your excel or Google sheet file as a .csv file. The first module of an analysis I produce is a plain .R script that loads in my .csv data file and save it in my R environment as a tibble, a type of data frame, using the new_tbl &lt;- read_csv(&quot;filepath&quot;) format. Before you read in a file, you should load the packages that we will be required for every analysis you conduct using the library(tidyverse) function. Note that you should not use the base R function read.csv but rather use the tidy-verse function read_csv. The base version will inevitably cause frustration due to incorrect variable class assignment. Method 2:From Hakai database It is possible to get data from the Hakai database directly into R without having to leave R Studio. This is accomplished by interacting with an application programming interface (API) we wrote specifically for downloading data from Hakai’s data portal. Earlier in the Data Analysts Toolbox section we installed Hakai’s API. Now we will use it to get Hakai data from the database. Below is a quickstart example of how you can download some chlorophyll data. To be able to access Hakai data you must sign in with a Hakai email address that has the appropriate permissions to access the database. This authentication is done using a token. When you run the code below, a web URL will be library(&#39;hakaiApi&#39;) # Get the api request client, run this line independently before the rest of the code client &lt;- hakaiApi::Client$new() # Follow stdout prompts to get an API token # Make a data request for chlorophyll data endpoint &lt;- sprintf(&quot;%s/%s&quot;, client$api_root, &quot;eims/views/output/chlorophyll?limit=50&quot;) data &lt;- client$get(endpoint) # Print out the data print(data) By running this code you should see chlorophyll data in your environment. The above code can be modified to select different datasets other than chlorophyll and filter based on different logical parameters you set. This is accomplished by editing the text after the ? in &quot;eims/views/output/chlorophyll?limit=50&quot;. To read about the API and how to use it for your first time go here. The list of endpoints for different data sets can be found here. The formula you set after the question mark is known as query string filtering. To learn how to filter your data read this In practice, I set the endpoint to the data table I’d like to retrieve and leave the default so I am returned the 20 most recent entries. I then look at the data file and remind myself what the exact names of the columns are so that I can construct the querystring filter so I can download the exact data I’d like. Once I have my data set in my work environment, I’ll make sure that all the variable classes are what I want them to be. Luckily, when read_csv reads in your file, it prints a summary of the name and type of each variable in your data frame. I always check: that my Date column has the class ‘date’ that categorical variables are stored as factors anything that is not a categorical variable is not stored as a factor but rather is numeric, character, or integer for example. One of the most common problems of why you get errors in R is from R assigning incorrect variable classes. Often you want your variable to be of class numeric, or character, but it is assigned as a factor. This is extremely common and can be remedied by coercing your variable class. Tidy your data Tidy data is best explained by a quote from Hadley Wickham’s paper in the Journal of Statistical Software: “Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” The ‘tidy-verse’ model of working in R that I have previously referred to relies on this system of data preparation. The basic tenets that make a data set tidy are: Each variable must have its own column Each observation must have its own row Each value must have its own cell Beyond that each type of observational unit should be its own table. That is to say, I wouldn’t mix weather metadata from field collection of a fish sample in the same table as the results of a laboratory analysis. Instead I would keep these two types of information, in different tables, but provide a common column in each that allows the tables to be later joined. This last part related to joining tables is an important concept that I will come back to. If you are involved in producing the data set, I recommend setting up your data collection to follow the tidy data format, to make your life, or the life of your analysts, much easier! If your data is not tidy, there are a variety of functions you can use from the tidyr package that you can learn about in Chapter 9: Tidy Data with tidyr. But hopefully your data has arrived to you tidy, in which case you can move on to processing the data for your analysis. Process your data There is usually some number of things that you will want to do to your data before you start exploring it. You may want to create new variables based on others, you may want to select only certain variables, or you may want to filter out certain observations. This would be to the time to get your data to where you want it before you start to analyze it. Select certain observations using select(), filter observations using filter() Exercises Set up a new Git Hub repo called ’developing-analyses-w-hakai Start a new R-Studio project Re-create the suggested template folder structure in the working directory you established your R project file in. Download this example dataset into the raw data folder of your project directory Make sure the data is tidy Process your data for how you want to analyze it and export it using the write_csv function to your processed data folder. 4.2.5 Exploratory data analysis This is where you start playing with the data. You will likely try many different summaries and visualizations while you seek a deeper understanding of your data. I typically create an exploratory data analysis (EDA) script specifically for this. I use R Markdown files (.Rmd) for my EDA. This allows me to weave narrative explanations of what exploration I am doing in each separate code chunk. Writing this narrative about why you chose to explore a certain analysis or summary, and why it did or didn’t work is key in documenting your analysis so you can come back to it, make sense of what you’ve done, and pick up where you left off. EDA allows me to try many different summaries or visualizations without having to worry about making the output perfect or pretty. This process will give you insight to your data as well as generate new questions and provoke new analyses. One excellent tool that can help you explore your data interactively is using shiny apps. There’s too much detail to focus on how to produce shiny apps here but suffice to say that creating interactivity with your data set is an excellent way to explore many different relationships between your data. Exploratory analysis can probably go on forever depending on the complexity of your data. You will probably think of 1000 different ways to look at your data, but after some time you need to refocus on your original question and think carefully about what analyses and visualizations are best to communicate. Because you’ve been storing all of your EDA in and EDA script you’ve got a record of every path you went down while exploring your data. You can then cherry pick the analyses, plots, and summaries you want to include in your final scripts and simply copy and past them into a new .Rmd script. Exercises Create a to do list using issues in Git Hub Produce summary plots and commits along with milestones Make a pull request Make a branch to produce a shiny app Make a pull request to merge shinny app Produce a shiny app 4.2.6 Predict and Model 4.2.7 Communicate What your final data product is going to be (summary report, figures for your manuscript, an interactive dashboard for a website), will dictate what your final scripts will be. As a baseline I recommend .Rmd as the final format because this gives you a lot of flexibility in terms of polished data products. 4.3 Programming style guide You’ll notice that throughout the example, there was consistent formatting of the code. Using consistent formatting for writing your code, makes it easier for everyone to interpret. I recommended the style guideline developed by Hadley Wickham called The tidyverse style guide. 4.4 Git flow Git Flow is a workflow that should generally be adhered to whether working individually or collaboratively. Some will argue that there’s no such thing as working individually, especially when you consider that you’re constantly collaborating with past-you, and future-you. The main concepts of Git Hub Flow are: Branching - This is a core concept in Git Hub and version control in general – The master branch must always remain stable and working. When you create a new branch, you are creating an environment where you can try out new ideas without directly manipulating the stable ‘master’ version of your analysis. Commits - Commits represent significant changes to your branch. I think of them as mini-milestones that add up to your complete analysis. Perhaps you’re mini-milestone is to create a linear model of something. You can work locally and make save files on your computer while working on the linear model like you normally would. When you finish your linear model, this is a good time to make a commit to your branch. You must name each commit, and this should be descriptive. In this case ‘fitted linear model’ would be ideal. Pull Requests - I like to think of pull requests as an opportunity to review and discuss the sum of several commits before they are merged back into the main branch. Something that helped clarify what a pull request is for me, was thinking about the difference between a push (write) request, and a pull (read) request. In the context of a shared repository, a pull request gives everyone that the repo is shared with a change to read your request to consider the sum of your mini-milestones. Pull requests should amount to the completion of a component of your project. See here for more info. Issues - When working ‘individually’, an excellent way to create a TODO list for yourself is to create new ‘Issues’. Again this could be something routine, like ‘filter data to include only sampling events in the morning.’ You can then leave yourself, and potential collaborators, a little note about why you’re doing this which will be preserved in a timeline for future you, or collaborators, to inspect. Each issue in your repo is given a number, and future commits can be labelled ‘fixes issue #45’ and when that commit is wrapped up in a pull-request which is eventually merged to the master branch, the issue will be marked as resolved. When working collaboratively, issues can be created by whomever is reviewing your code, and the issue can be assigned to you to fix. Merge - Once a Pull Request has been sufficiently reviewed, discussed, and tested (deployed), the new code can be merged with the master branch, typically by the team owner, or maintainer. See this GitHUB Flow help article for a nice walk through with good diagrams. 4.4.1 Final data product What your final data product is going to be (summary report, figures for your manuscript, an interactive dashboard for a website), will dictate what your final scripts will be. As a baseline I recommend .Rmd as the final format because this gives you a lot of flexibility in terms of polished data products. 4.5 Collaboration with phyton users The data analysis files you commit to your repo should include a ‘feather’ table. Feather is data frame format that is readable by both Python and R, and therefore every analysis should produce this type of data frame to ensure reproducibility for Python users "],
["additional-references-and-learning-resources.html", "5 Additional References and Learning Resources", " 5 Additional References and Learning Resources R Markdown Shiny Advanced R UCLA Fisheries and R "],
["glossary-of-terms.html", "6 Glossary of Terms", " 6 Glossary of Terms Branching: Commit: Developing Analyses: Distributed Version Control: Issues: Local: Merge: Remote: Pull Request: Push: Version Control: Working Directory: "]
]
