[
["guidelines-for-hakai-data-analysts.html", "3 Guidelines for Hakai Data Analysts 3.1 Reproducible Research, Literate Programming, and the Open Notebook 3.2 Programming Style Guide 3.3 Organization of a Data Analysis Workflow 3.4 GitHUB Best Practices 3.5 Reproducible research using markdown, notebooks, or other (Need significant additions, editing, and contributions beyond here) 3.6 Collaboration with Phyton users", " 3 Guidelines for Hakai Data Analysts 3.1 Reproducible Research, Literate Programming, and the Open Notebook These three concepts bring together a very modern way of conducting science. These are the benefits of using these methods: They save you work in the long run by being able to reproduce your own analyses after you’ve long forgotten the details of how they were conducted If you weave a narrative text into your computer code you’ll be able to understand what you were thinking at a later time when you revisit it. You can easily collaborate show your peers that you have nothing to hide in your analytical methods that you are willing to share your analyses in hopes that others will improve the quality of your analysis by offering their insight. Watch: Dr. RD Peng’s youtube video on reproducible research. As well as his video on Literate Programming Open notebook science involves providing all of your research thoughts and ideas from within your notebook for maximum transparacey. 3.2 Programming Style Guide Google’s R Style Guide is an excellent place to start. Googles guide recommendeds two space indents for new lines of code within the same function, but I prefer 8. See Roger Pengs Coding Standards Video for some excellent suggestions that I mostly adhere to. 3.3 Organization of a Data Analysis Workflow BJ TODO: Insert section on r projects template I keep all files associated with an analysis in a google drive folder, so that I can give collaborators access to RAW data so they can work with it directly. I use a default folder structure for every analysis based on the files that are typically produced from every analysis. First I create a folder for the name of the project and create these sub-folders within the project folder. The sub folders I use are: * raw data * processed data * raw scripts * final scripts * exploratory figures * final figures * notebook I typically have my notebook folder set as my working directory and navigate from there to save figures, processed data etc. using the “../processed data/yourfilename.csv”. The .. before the forward slash tells R to navigate up one level from your working directory, and then down one level into, in this case, your processed data folder. 3.4 GitHUB Best Practices 3.4.1 GitHUB Flow GitHUB Flow is a workflow that should generally be adhered to whether working individually or collaboratively. Some will argue that there’s no such thing as working individually, especially when you consider that you’re constantly collaborating with past you, and future you. The main concepts of GitHUB Flow are: Branching - This is a core concept in GitHUB and version cntrol in general – The master branch must always remain stable and working. When you create a new branch, you are creating an environment where you can try out new ideas without directly manipulating the stable ‘master’ version of your analysis. Commits - Commits represent significant changes to your branch. I think of them as mini-milestones that add up to your complete analysis. Perhaps you’re mini-milestone is to create a linear model of something. You can work locally and make save files on your computer while working on the linear model like you normally would. When you finish your linear model, this is a good time to make a commit to your branch. You must name each commit, and this should be descriptive. In this case ‘fitted linear model’ would be ideal. Pull Requests - I like to think of pull requests as an opportunity to review and discuss the sum of several commits before they are merged back into the main branch. Something that helped clarify what a pull request is for me, was thinking about the difference between a push (write) request, and a pull (read) request. In the context of a shared repository, a pull request gives everyone that the repo is shared with a change to read your request to consider the sum of your mini-milestones. Pull requests should amount to the completion of a component of your project. Issues - When working ‘individually’, an excellent way to create a TODO list for yourself is to create new ‘Issues’. Again this could be something routine, like ‘filter data to include only sampling events in the morning.’ You can then leave yourself, and potential collaborators, a little note about why you’re doing this which will be preserved in a timeline for future you, or collaborators, to inspect. Each issue in your repo is given a number, and future commits can be labelled ‘fixes issue #45’ and when that commit is wrapped up in a pull-request which is eventually merged to the master branch, the issue will be marked as resolved. When working collaboratively, issues can be created by whomever is reviewing your code, and the issue can be assigned to you to fix. Merge - Once a Pull Request has been sufficiently reviewed, discussed, and tested (deployed), the new code can be merged with the master branch, typically by the team owner, or maintainer. See this GitHUB Flow help article for a nice walkthrough with good diagrams. 3.5 Reproducible research using markdown, notebooks, or other (Need significant additions, editing, and contributions beyond here) R Notebooks is a good way to collaborate with other analysts. When using R Notebooks it’s best to set defualts to: You want your YAML header to look something like this: output: html_notebook: default, keep_md: TRUE github_document: default 3.6 Collaboration with Phyton users The data analysis files you commit to your repo should include a ‘feather’ table. Feather is data frame format that is readable by both Python and R, and therefore every analysis should produce this type of dataframe to ensure reproducibility for Python users "]
]
