[
["index.html", "Hakai Institute R Data-Analyst Guide Preface", " Hakai Institute R Data-Analyst Guide Brett Johnson Latest update: 2017-08-29 Preface This is the Hakai Institute’s Data Analyst Guide. One of Hakai’s core themes that cross-cuts each research axis is ‘Big-Data and Modelling’. Our ability to gain insight from our long-term ecological data sets is limited only by our capacity to integrate, and digest data. To that end, this guide was developed to serve as a foundation from which to build the internal technical capacity — using a coherent and systematic approach — to turn data into insight, understanding, and ultimately knowledge about the ecosystems we study. This book will teach you how to develop analyses in the R programming language. You’ll learn the basics, project workflow and organization, literate programming style, reproducible research techniques, and collaborative analysis development in an open-science context. The Hakai Institute is a research organization based on the coast of British Columbia, Canada. See the Hakai Institute Website for more information. "],
["introduction.html", "1 Introduction 1.1 Filling in the Gaps 1.2 Modern Approach to Data Analysis 1.3 Acknowledgements", " 1 Introduction 1.1 Filling in the Gaps Most University courses that use R are focused on teaching statistical techniques such as generalized linear models, or logistic regression — which is very necessary and important — but they pay no attention to educating students on how to actually conduct an analysis from start to finish. Questions that often remain un-answered include: What is an efficient workflow? How do I access and import data? How do I clean and manipulate my data into a format to analyze? How do I maintain version-control of my analysis? How do I make my analysis reproducible in case I get new data or someone else wants to run my code? How can I collaborate on this analysis? How can I get my analysis into a format for someone to meaningfully conduct peer-review? How do I efficiently produce a professional report or other artifact of my analysis to communicate results? This guide aims to bridge this gap by taking you through the steps to produce a well-developed analysis that will form a template for conducting your own analyses while working with Hakai. You will be directed through an efficient way to learn teh basics of R, and progress to modern methods of communicating results. Learning to develop analyses in R can be a very frustrating and difficult experience. Know that we have all suffered severely in this way, but that this guide will hasten you through the drudgery and frustration. No computer programming experience is required to work through this guide, though an upper level undergraduate course in ecological statistics should be considered pre-requisite. Combined with a graduate level understanding of statistics, this guide will allow you to develop very modern analyses. 1.2 Modern Approach to Data Analysis There is a vibrant ‘open-source’ community of people developing methods, packages, and workflows in the R programming world. Consequently, we have some of the most modern, flexible, and high-level methods to develop and communicate statistical analyses. However, as the R community iterated ideas of how the programming language works at a very base level over the last couple of decades, we are left with a litany of methods and programming syntax. There are typically at least half a dozen ways to write a chunk of code to reach the same desired result in R. This, in part, has given R the reputation as being difficult to learn. This guide aims to address the ‘thousand and one way of doing things’ problem in R by focusing on the recent development of packages that form a simple, elegant, and coherent grammar of data analysis. This collection of packages and methods is known as the ‘tidyverse’, developed in large part by the Chief Scientist of R-Studio, Hadley Wickham, and has recently been adopted as the best way forward for academic applications of the R language. The objectives of this book are to serve as a: Guide to install, set-up, and become familiar with analysis development tools; R, R-Studio, Git and Git Hub. Best-practice guidelines to develop reproducible, accurate, and collaborative analyses in R-Studio. 1.3 Acknowledgements Much of this document refers you to material that others have worked very hard to make. I simply point to these resources in an order that makes sense and seems systematic to me. Many thanks to the following people for making this possible: Dr. Jenny Bryan, Data Science Professor at UBC, RStudio Twitter, GitHUB. Dr. Roger Peng, Professor of Bio-statistics at Johns Hopkins University. Website, GitHUB. Dr. Hadley Wickham, Chief Scientists at R-Studio. Website, Twitter, GitHUB. Dr. Hillary Parker, Stitch-fix, Twitter "],
["data-analysts-toolbox.html", "2 Data Analyst’s Toolbox 2.1 R and R Studio 2.2 Version control with Git and GitHub", " 2 Data Analyst’s Toolbox 2.1 R and R Studio R is the statistical programming language that R Studio provides and interface to. I think of R Studio as being the front-end user interface to the coding language R which works in the background. For this all to work you you need to download two things to start; R and R Studio. Install R: Go to https://cran.r-project.org/ and follow the link to the download for your operating system. Install R Studio: Go to https://www.rstudio.com/products/rstudio/download/ to download R studio. Now that you have downloaded the tools you need, lets get familiar first with R-Studio and then the R programming language. 2.1.1 Step-by-step R/RStudio R Studio Familiarization: Watch this R-Studio Tutorial for a nice introduction to the software. Start Plotting: Now that you’re familiar with the software, lets get your feet wet in R and R-Studio. I recommend you start making some plots by working through the following exercises: Section 3.1 to 3.4 in Hadley Wickham’s R for Data Science Book. Projects Workflow: Watch R-Studio tutorial: Projects and R Studio Install Hakai’s Application Programming Interface This API is installed directly in R-Studio and allows you to download data from the Hakai Database. Open up R Studio and in the console window copy and paste the following code and hit enter: install.packages(&#39;devtools&#39;) install.packages(&#39;tidyverse&#39;) library(&#39;devtools&#39;) library(&#39;tidyverse&#39;) devtools::install_github(&quot;HakaiInstitute/hakai-api-client-r&quot;, subdir=&#39;hakaiApi&#39;) 2.2 Version control with Git and GitHub Version control is an additional level of saving your files. There’s no need to have multiple versions of the same file on your computer with different dates or initials to identify the version you want to work on (eg. fishy_analysis_V9_2017_05_11_BJ_Bad_Scoobies.R, etc…) — this is what version control using Git and GitHub is for. The additional ‘save’ that comes from version control is known as a commit. You save your files like you normally would, but every once in a while you commit your files as an official version to be remembered. A commit can be thought of as a bullet point in the to do list of your analysis, and each commit you make must be accompanied by a message. For example; ‘read in data and tidy it up’, or ‘remove observations from non-standard sampling event, and re-fit GLM’. Git tracks the commits you make in R-Studio locally on your own computer. When you are ready for a series of commits to be made public, you push your commits to your remote repository at GitHub. Similar to the relationship between R and R-Studio, Git is the local version control system running in the background on your computer while GitHub is the remote user interface for saving, tracking, and sharing updated versions of your analyses that are hosted on a remote server. Git and GitHub are integral for tracking the evolution of a set of files, and ultimately the development of your analysis. If you’re interested in learning why version control is important, I encourage you to skim Chapter 1; ‘Why Git? Why GitHub’ from Jenny Bryan’s Book. I recommend referring back to this book whenever you have a question about using git and GitHub with R-Studio. You can also watch this video for an introduction to Git and GitHub. 2.2.1 Step-by-step Git/GitHub Setup a GitHub Account: First, you’re going to want to sign up for a GitHub account Install and set-up Git: Installing Git locally and getting Git to communicate to your remote GitHub website account, and then getting them to talk with R Studio takes a number of steps to complete…This next part can be painful to work through, but it is 100 % necessary and well worth it in the long-run. Install Git and GitHub and get them talking to R-Studio using Jenny Bryan’s Guide, start at chapter 7 and work to the end of chapter 13. Put this all in context: To learn how to use Git, GitHub and R-Studio I recommend watching the R-Studio Essentials tutorial; GitHub and R-Studio. If you are working directly for Hakai, you should contact the IT department: eims@hakai.org, to request that you be brought on to the Hakai GitHub team so that your work is stored in the Hakai repositories rather than on your personal account. Congratulations you now have setup the main tools in the R Analyst toolbox: R, R Studio, Git and GitHub and learned the basics of each component. In the next chapter you will get an introduction working with R. "],
["intro-to-r.html", "3 Intro to R 3.1 Primary Resource 3.2 Data Transformation 3.3 Tidying Your Data 3.4 Visualizing Data 3.5 Learning Objectives", " 3 Intro to R In this section you will learn the basics of data wrangling in R. Data wrangling is the process of importing, tidying, and transforming your data into a format that you can visualize and model it. Tidy data is a consistent data format that, when followed closely, dramatically reduces the mundane data formatting that is necessary otherwise. The basic concept is that every row is one observation and every column is a variable. For how simple it sounds, creating tidy data sets takes some time to get the hang of initially, but is essential for data wrangling in the tidyverse — the modern set of tools and packages developed in R around the concept of tidy data. 3.1 Primary Resource The ‘bible for a new generation of Data Scientists’ is Hadley Wickham and Garrett Grolemund’s Book: R For Data Science. This book presents a modern approach to data analysis and leads you to master the tidyverse; a combination of R packages and a well thought out and systematic approach to “import, tidy, transform, visualize, and model data.” Using the tidyverse as a foundation for your coding replaces the ‘thousand and one ways’ of doing things in R into a modern and concise grammar of data analysis development. 3.2 Data Transformation The most important set of tools for data wrangling that you will use constantly are from the package dplyr. They allow you to solve almost all of the data manipulation problems you will encounter. Those who are able to quickly manipulate their data into a format they can visualize and model, will explore their data the most and therefore learn the most. I’d recommend learning these six data wrangling functions off-by-heart: filter() to pick observations (rows) based on their values select() to pick variables (columns) based on the names arrange() to reorder the rows of your data set mutate() to create new variables (columns) as a function of existing variables summarize() to collapse many values down to a summary group_by() to summarize based on the various groups of a variable To learn how to put this in to practice I recommend reading through the chapter called Data Transformation in R for Data Science and working through all the exercises. This will be a bit of a learning curve but it’s time to dive in! Starting at the Data Transformation chapter assumes you have some very basic knowledge of R already. If that’s not the case then I recommend starting at the Workflow: Basics chapter. 3.3 Tidying Your Data Tidy data is best explained by a quote from Hadley Wickham’s paper in the Journal of Statistical Software: “Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” The basic tenets that make a data set tidy are: Each variable must have its own column Each observation must have its own row Each value must have its own cell All of the data stored in Hakai’s database is designed to be tidy, but sometimes you will receive data that is not in a tidy format. The most common problem I run into is that not every row is one observation, it is often many more than one because observations are stored as variables. To get your data into a tidy format you have to spread out the observations into different rows and create new variables that were once values. Luckily, the package tidyr has several functions that make this possible. To get a really strong handle on what tidy data is, and to start practising getting data in to a tidy format I recommend working through the chapter on Tidying Data in the R for Data Science book. 3.4 Visualizing Data In the last chapter you had a teaser of what is possible in terms of making some nice looking plots when you worked through the first few sections in the Data Visualization chapter in R for Data Science. To see what else is possible and to learn the principles of the layered grammar of graphics, I recommend you work through the remaining sections in the Data Visualization Chapter picking up where you left of at the sub-section on facets By the end of this you should have a really solid foundation from which to begin wrangling and visualizing your own data. In the next chapter I’ll provide some gerenal principles for developing analyses. 3.5 Learning Objectives Before moving on to the next chapter, work through these chapters mentioned above and complete their respective exercises: Workflow: Basics Data Transformation Tidying Data Data Visualization I highly reccommend working through all the chapters in this book at some point, but these first few chapters are a good start. "],
["general-principles.html", "4 General Principles 4.1 Reproducible Research 4.2 Open Science Collaboration", " 4 General Principles New ideas about what makes a good data analysis are emerging. With data being so readily available in vast quantities, analyzing data using out of date methods — such as Microsoft excel — can quickly become overwhelming, not reproducible, error-prone, and difficult to assess for reliability. Much of the progress in terms of ‘developing analyses’ has been made in the field of bio-statistics due to the high volume of genomic data that researchers deal with. One of the most concerning examples of what can go wrong with an analysis, is from the field of genomics and cancer treatments. In the ‘Duke Scandal’, researchers made mistakes in their data analysis, that were extremely difficult to track, and resulted in patients receiving the wrong cancer treatment. This is an extreme example that affected peoples lives directly. I would argue, that the work that we do at Hakai, analyzing ecological data, has much broader implications and should be treated with an equal degree of discretion. Some important concepts in defining a good data analysis are: Reproducible Research, Open Science Collaboration These two concepts bring together a very modern way of conducting science that are beneficial in the following ways: An additional peer review process becomes possible in the development of your analyses—peers can review how you conducted your analysis You can reduce your work in the long run by being able to reproduce your own analyses after you’ve long forgotten the details of how they were conducted If you weave a narrative text into your R code you’ll be able to understand what you were thinking at a later time when you revisit it You can meaningfully collaborate You can show your peers that you have nothing to hide in your analytic methods, immediately increasing the reliablilty of your work You can share your analyses in hopes that others will improve the quality of your analysis by offering their insight 4.1 Reproducible Research A big concern with modern scientific studies is the inability to reproduce study findings. As experimental and analytic methods become increasingly complex, published methods often lack the detail required to be able to reproduce the study. As data analysts, it is our responsibility to include in our analyses sufficient notation to facilitate deep understanding of what was done. If your study finds something very interesting, people are going to want to know how you came to your conclusion. The gold standard for verifying a study is to independently replicate it. However, before investing the huge amount of resources required to replicate a study independently, a better place to start is to reproduce the study findings using their data and analysis. In order for your study and your analysis to be reproducible and to be viewed as trustworthy, you need to be able to provide the data, the scripted code you used to clean, summarize, model, and visualize that data, and then a reviewer has to be able to run that same code and see the same results. This level of transparency allows a reviewer to look very closely at how you conducted your analysis. This adds an additional step in the peer review process which has not previously been possible with un-scripted analyses. A simple example of how easy it is to break the reproducibility chain comes from working with your data in excel. By simply deleting some values that looked to be outliers, without recording anywhere that you did that, or why you did that, you have effectively broken the reproducibility chain. Another person could not receive the raw data and come to the same conclusions as you did — their results would be different because of the missing data. Some academic journals, such as the Journal of Bio-statistics, are taking proactive steps by adopting a reproducibility policy for article submissions. The journal of Nature has taken a stance on reproducibility and has published several articles in a special called Challenges in Irreproducible Research. If you google reproducible research you will find many initiatives that are attempting to solve this important problem. Most often, the peer-reviewer or collaborator that you will work with is ‘future-you’. Scripting reproducible analyses with embedded narrative allows future-you to understand what past-you was thinking. This, in turn, saves you a lot of time, and allows you to build upon your previous work. The idea of embedding your own narrative, or adding comments to your code, introduces the idea of literate programming. Weaving together human-readable narrative text that explains what your computer code is doing and why you decided to do it, greatly increase the quality of your work. For some comic relief on the follies of working with someone who doesn’t use reproducible research methods watch this youtube video 4.2 Open Science Collaboration 4.2.1 Peer Review Open science collaboration establishes an additional peer-review step in the scientific process. By making your analysis public using a distributed version control system like GitHub you open up the possibility for on-going peer-review of your analysis, as well as the the opportunity for not only your immediate colleagues, but also experts in your field to contribute in a meaningful and formalized way. 4.2.2 Code Review Before you publish or distribute your analysis, you must have a colleague reveiw your code. You would never publish a written article without sending it to a trusted co-worker or superviser first. Code review is the same thing. While peer-review permits the scientific merit of your analyses to be assesed, code-review permits a colleague to assess your analyses for coding errors. Developing analyses is difficult and error prone. Using the distributed version control system GitHUB, you are able to track changes to your code over time and enable others to suggest edits for your review, leaving you with a history of exactly what was added when and by whom — this is version control. The distributed aspect of ‘distributed version control’ means that multiple people can access the version control through your repository of all the files of your analysis (and their previous versions). You can make your repository (repo) private or public. By acessing your repo, which is hosted on GitHub, others can meaningfully collaborate, conduct peer-review, and code-review. Using consistent formatting for writing your code, makes it easier for everyone to interpret. I recommended the The tidyverse style guide. Have a read through the guide and try to implement in for your next script. To check that your script follows the tidyverse style guide, a code formatting ‘spell-check’ package called lintr() comes in handy. In the R console simple type: install.packages(&quot;lintr&quot;) library(lintr) lint(&quot;/path_to_your_file.R&quot;) Lint will take you row by row of your code and tell you how you deviated from the tidyverse style guide. 4.2.3 Collaboration Software developers have been using this type of development platform for some time. By using these tools, data analysis can progress towards the concept of developing analyses, as opposed to an ad-hoc analysis. Ultimately, this approach makes your life easier. If you’ve experienced the pain of digging through your emails or zip files to find the analysis you or your colleague conducted years ago, only to be left perplexed by which version is the ‘master’ or ‘final’ then Git HUB will be a welcome addition to your toolbox. GitHub can also be used as a project management tool for example by creating to-do lists for yourself and your team. By creating issues you can leave yourself a to-do list, or collaborators can identify issues with your analysis. We will cover Git HUB fundamentals in the next chapter. "],
["developing-analyses-with-the-hakai-institute.html", "5 Developing Analyses with the Hakai Institute", " 5 Developing Analyses with the Hakai Institute This chapter is currently under development. "],
["learning-resources-and-reference-material.html", "6 Learning Resources and Reference Material", " 6 Learning Resources and Reference Material The one and only introductory R book to read from front to back: R For Data Science Functional Programming (Loops and stuff) Purr Tutorial Advanced R Git and GitHub Happy Git with R by Jenny Bryan Plots Cookbook for R Statistical Modeling Examples UCLA Institute for Digital Research and Education Statistics Undergraduate Biologist’s Guide with R Examples Tidyverse Tidyverse Website Tidyverse style guide "],
["glossary-of-terms.html", "7 Glossary of Terms", " 7 Glossary of Terms Branching: From GitHUB: “A branch is a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the”live&quot; version. When you’ve made the changes you want to make, you can merge your branch back into the master branch to publish your changes.&quot; Commit: From GitHub: “A commit, or”revision“, is an individual change to a file (or set of files). It’s like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the”SHA&quot; or “hash”) that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made.&quot; Developing Analyses: A term coined by Hillary Parker that describes the process by which a data anlaysis is ensured to be reproducible, accurate and collaborative. Distributed Version Control: A system of tracking the changes in a set of files that allows devlopers to work on locally stored copies of the same set of files, before sharing the changes with a remote copy of the set of files. Fork:From GitHub: “A fork is a personal copy of another user’s repository that lives on your account. Forks allow you to freely make changes to a project without affecting the original. Forks remain attached to the original, allowing you to submit a pull request to the original’s author to update with your changes. You can also keep your fork up to date by pulling in updates from the original.” Issues: From GitHub: “Issues are suggested improvements, tasks or questions related to the repository. Issues can be created by anyone (for public repositories), and are moderated by repository collaborators. Each issue contains its own discussion forum, can be labeled and assigned to a user.” Local: The set of your repositories files that are stored on your own computer. Literate Programming: A programming paradigm invented by Donals Knuth that puts the emphasis on human readability of code and flow of code structure that is logical to the human. Merge: From GitHub: “Merging takes the changes from one branch (in the same repository or from a fork), and applies them into another. This often happens as a pull request (which can be thought of as a request to merge), or via the command line. A merge can be done automatically via a pull request via the GitHub web interface if there are no conflicting changes, or can always be done via the command line. For more information, see”Merging a pull request.&quot; Remote: From GitHub: “This is the version of something that is hosted on a server, most likely GitHub. It can be connected to local clones so that changes can be synced.” Pull Request: From GitHub: “Pull requests are proposed changes to a repository submitted by a user and accepted or rejected by a repository’s collaborators. Like issues, pull requests each have their own discussion forum.” Push: From GitHub: “Pushing refers to sending your committed changes to a remote repository, such as a repository hosted on GitHub. For instance, if you change something locally, you’d want to then push those changes so that others may access them.” Working Directory: The folder in which your R session is reading and writing files from and to. "]
]
