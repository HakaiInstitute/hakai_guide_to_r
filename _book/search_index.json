[
["index.html", "Hakai Institute R Data-Analyst Guide Preface", " Hakai Institute R Data-Analyst Guide Brett Johnson 2017-05-29 Preface The Hakai Institute is a research organization based on the coast of British Columbia, Canada. See the Hakai Institute Website for more information. "],
["introduction.html", "1 Introduction 1.1 Acknowledgements", " 1 Introduction The objective of this guide is to serve as a: Reference guide for the new R Analysis Developer; guidelines for Hakai staff to develop reproducible, accurate, and collaborative analyses in R-Studio. There is a vibrant ‘open-source’ community of people developing methods, packages, and workflows in the R programming world. Consequently, we have some of the most modern, flexible, and high-level methods to develop and communicate statistical analyses. However, as the community iterates ideas of how the programming language works at a very base level, we are left with a littany of methods and programming syntaxes. There are typically atleast half a dozen ways to write a chunk of code to reach the same desired result in R. This, in part, has given R the reputation as being difficult to learn. This guide aims to address the ‘thousand and one way of doing things’ problem in R by focussing on the recent development of packages that form a simple, elegant, and coherent grammar of data analysis. This collection of packages and methods is known as the ‘tidyverse’, developed in large part by the Chief Scientist of R-Studio, Hadley Wickham. Most Univeristy courses that use R are focussed on teaching statistical techniques such as Generalized Linear Models, or Logistic Regression, but pay no attention to educating students on how to actually conduct an analysis from start to finish. Questions that often remain un-aswered are: What is an efficient workflow? How do I get data? How do I clean data? How do I make my analysis reproducible in case I get new data or someone else wants to run my code? How can I collaborate on this analysis? How can I get my analysis into a format for someone to meaningfully conduct peer-review? How do I maintain version-control of my analysis? How do I efficinetly produce a professional artifact of my analysis to distribute? This guide points to solutions that adress these questions by providing an example of a well-developed analysis. As a new analyst you will be directed through an efficient way to learn R, and progress to modern methods of communicating resluts. Combined with a graduate level understanding of statistics, this guide should get you to an intermediate skill level in R-Studio using a very high-level, and modern approach. 1.1 Acknowledgements Much of this document refers you to material that others have worked very hard to make. I simply point to these resources in an order that makes sense and seems systematic to me. Many thanks to the following people for making this possible: Dr. Jenny Bryan, UBC Professor in the Masters of Data Science Program. Twitter, GitHUB. Dr. Roger Peng, Professor of Biostatistics at Johns Hopkins University. Website, GitHUB. Dr. Hadley Wickham, Chief Scientists at R-Studio. Website, Twitter, GitHUB. "],
["defining-a-good-data-analysis.html", "2 Defining a Good Data Analysis 2.1 Reproducibility 2.2 Version Control 2.3 Communication and Distribution 2.4 Open Source and Open Science 2.5 Resources", " 2 Defining a Good Data Analysis Defininig a good data analysis is nearly impossible. However, new ideas about what makes a good data analysis are emerging. With data being so readily available in vast quantities now — analyzing data using out-of-date methods such as microsoft excel, quickly becomes overwhelming, not reproducible, error-prone, and difficult to assess for reliability. Much of the progress in terms of ‘developing analyses’ has been made in the field of bio-statistics due to the high volume of genomic data that researchers deal with. One of the most concerning examples of what can go wrong with an analysis, is from the field of genomics and cancer treatments. In the ‘Duke Scandal’, researchers made mistakes in their data analysis, that were extremely difficult to track, and resulted in patients receiving the wrong cancer treatment. This is an extreme example that affected peoples lives directly. I would argue, that the work that we do at Hakai, analyzing ecological data, has much broader implications and should be treated with an even higher degree of discretion. Some important concepts emerging in defining a good data analysis are: 2.1 Reproducibility If your study finds something very interesting, people are going to want to know how you came to your conclusion. A simple example of the reproducibiity concept is cleaning your data in excel. By simply deleting some cells that looked to be outliers, without recording anywhere that you did that, or why you did that, you have effectively broken the reproducibility chain. Another person could not come to the same conclusions as you did, if you provided me the raw data set you started with. In order for your analysis to be trustworthy, you need to be able to provide the data, the scripted code you used to clean, summarize, analyze, and plot that data, and then a reviewer has to be able to run that same code and see the same results. This level of transparency allows a reviewer to look very closely at how you conducted your analysis. This adds an additional step in the peer review process which has not previosuly been possible with un-scripted analyses. The Journal of Biostatistics has adressed many of these important issues by develping a policy around reproducibility and released an article called ‘Reproducible research and Biostatistics’ The reality is, the peer-reviewer or collaborator that you will most often want to work with is ‘future-you’. Scripting reproducible anlayses with embedded narrative allows future you to understand what past-you was thinking. This, in turn, saves you a lot of time in the long run. The idea of embedding your own narrative, or adding comments to your code, introduces the idea of literate programming. By weaving together human-readbale narrative that explains what your computer code is doing and why you decided to do it, greatly increase the quality of your work. 2.2 Version Control 2.3 Communication and Distribution 2.4 Open Source and Open Science 2.5 Resources The ‘bible for a new generation of Data Scientists’ is Hadley Wickham and Garrett Grolemund’s Book: R For Data Science. This book presents a modern approach to data analysis and leads you to master the ‘tidyverse’; a combination of R packages and a well thought out and systematic approach to “import, tidy, transform, visualize, and model data.” Using the tidyverse as a foundation for your coding replaces the ‘thousand and one ways’ of doing things in R into a modern and concise grammar of data analysis development "],
["guide-for-the-new-r-analysis-developer.html", "3 Guide for the New R Analysis Developer 3.1 Install and Set up R and R Studio 3.2 Version Control with Git and GitHUB", " 3 Guide for the New R Analysis Developer 3.1 Install and Set up R and R Studio R is the statistical programming language that R Studio understands. I think of R Studio as being the front-end user interface to the coding language R which is running in the background. For this all to work you you need to download two things to start; R and R Studio. Watch my Youtube Video on how to install R and R Studio Go to https://cran.r-project.org/ and follow the link to the download for your operating system. Go to https://www.rstudio.com/products/rstudio/download/ to download R studio 3.1.1 Familiarize yourself with R and R-Studio Now that you have downloaded the tools you need, lets get familiar first with R-Studio and then the R programming language. 3.1.2 R Studio Watch this R-Studio Tutorial for a nice introduction to the software. 3.1.3 Making Plots Just to get your feet wet in R and R-Studio, I recommend you start making some awesome graphs by working through section 3.1 to 3.4 in Hadley Wickham’s R for Data Science Book 3.1.4 R Studio Projects Workflow Watch R-Studio tutorial: Projects and R Studio 3.2 Version Control with Git and GitHUB The reason this book is hosted on GitHUB is that we’ll be working with GitHUB throughout this guide and throughout your own analysis development. Git is integral for tracking the evolution of a set of files. Basically GitHUB and R is analagous to ‘track changes’ and Microsoft Word. If you’re interested in learning why version control is important, I encourage you to skim Chapter 1; ‘Why Git? Why GitHUB’ from Jenny Bryans Book. I reccomend referring back to this book whenever you have a question about using git and GitHUB with R-Studio. If you don’t feel like reading about Git and GitHUB watch this video for an introduction to Git ad GitHUB. 3.2.1 Setup a GitHUB Account First, you’re going to want to sign up for a GitHUB account 3.2.2 ‘Issues’ (Need to make this repo public for this to be possible) Now that you have a GitHUB account, you should introduce yourself and let me know you’re going to work through this material. To do that go to the issues tab in this repository (repo) and ‘create a new issue’ and type a message to introduce yourself. Using issues is also a great way to ask questions about course material, or suggest imrpovements to the guide. 3.2.3 Fork This Repo Next, you should fork this repo so that you can access this document from your GitHUB profile, edit it, and make a ‘pull request’ to suggest edits to me. This serves two purposes: 1) the quality of this guide will be improved if you take time to edit chapters and contribute your insights on what didn’t work for you; and 2) forking repos and making pull-requests is training you for the workflow you will use when contributing to colleagues’ analyses in the very near future. To successfully fork this repo and suggest edits simply go to my repo here and click on the name of this chapter: ‘new_analysts.Rmd’, once it’s open click on the pencil icon to suggest edits. Editing automcatically forks the repo and makes a copy in your own list of repos. Now you can make all the edits you’d like and when your happy you can click on ‘pull request’ to send the updates back to me so I can review them and decide if I should accept your edits and merge them. If you aren’t successfull forking my repo, read Chapter 24 of Jenny Brians Book; ‘Fork a Repo’. 3.2.4 Markdown Syntax You’ll notice that this file is a ‘.Rmd’ file which stands for R Markdown. Read this if you’re interested in markdown syntax, otherwise come back to it later. Also, try this tutorial on how to write using markdown 3.2.5 Install and set-up Git Next you’re going to install Git. Git tracks the changes you make in R-Studio locally and upon your request, pushes changes to your remote repository at GitHUB. Similar to the relationship between R and R-Studio, Git is the local version control system running in the back ground on your computer while GitHUB is the remote user interface for saving, tracking, and sharing updated versions of your analyses that is hosted on a remote server. Installing Git locally and getting Git to communicate to your remote GitHUB website account, and then getting them to talk with RStudio takes a number of steps to complete…This next part can be painful to work through, but it is 100 % neccesary and well worth it in the long-run. Install Git and GitHUB and get them talking to R-Studio using Jenny Bryan’s Guide, start at chapter 7 and work to the end of chapter 13. Congratulations you now have setup the three main tools in the R Analyst toolbox: R, R Studio and GitHUB and learned the basics of each component. Now to put this all in context, and learn how to use Git, GitHUB and R-Studio I reccomend watching the R-Studio Essentials tutorial; GitHUB and R-Studio. Now you’ve seen almost all the components of a good workflow, in the next chapter you can put all these things together and work through an example of a Data Analysis using a presecribed, modern approach. "],
["guidelines-for-developing-analyses-with-hakai-institute.html", "4 Guidelines for Developing Analyses with Hakai Institute 4.1 Reproducible Research, Literate Programming, and the Open Notebook 4.2 Programming Style Guide 4.3 Example Project 4.4 GitHUB Best Practices 4.5 Reproducible research using markdown, notebooks, or other (Need significant additions, editing, and contributions beyond here) 4.6 Collaboration with Phyton users", " 4 Guidelines for Developing Analyses with Hakai Institute 4.1 Reproducible Research, Literate Programming, and the Open Notebook These three concepts bring together a very modern way of conducting science. These are the benefits of using these methods: They save you work in the long run by being able to reproduce your own analyses after you’ve long forgotten the details of how they were conducted If you weave a narrative text into your computer code you’ll be able to understand what you were thinking at a later time when you revisit it. You can easily collaborate You can show your peers that you have nothing to hide in your analytical methods You can share your analyses in hopes that others will improve the quality of your analysis by offering their insight. 4.2 Programming Style Guide Using consistent formatting for writing your code, makes it easier for everyone to interpret. I loosely follow Google’s R Style Guide. This is an excellent place to start. But you can make slight modifications at your discretion, for example; Googles guide recommendeds two space indents for new lines of code within the same function, but I prefer 8. See Roger Pengs Coding Standards Video for some excellent suggestions that I mostly adhere to. 4.3 Example Project In this section I will walk you through an example of a project that brings together all the different concepts we’ve been talking about up to this point. This project should serve as an example of how to develop analyses for the Hakai Institute. I start every new project in the following manner. I first create a GitHUB repo, start a new R-Studio project and link it to my GitHUB repo, and then I populate my working directory with a template folder structure that stores the different categories of files I produce from every analysis. 4.3.1 Set up a repo on GitHUB From Jenny Bryan’s Book: “Do this once per new project. Go to https://github.com and make sure you are logged in. Click green “New repository” button. Or, if you are on your own profile page, click on “Repositories”, then click the green “New” button. Repository name: myrepo (or whatever you wish) Public YES Initialize this repository with a README Click the big green button “Create repository.” Copy the HTTPS clone URL to your clipboard via the green “Clone or Download” button. Or copy the SSH URL if you chose to set up SSH keys.&quot; 4.3.2 Start a new R-Studio Project From Jenny Bryan’s Book: “In RStudio, start a new Project: File &gt; New Project &gt; Version Control &gt; Git. In the “repository URL” paste the URL of your new GitHub repository. It will be something like this https://github.com/jennybc/myrepo.git. Be intentional about where you create this Project. Suggest you “Open in new session”. Click “Create Project” to create a new directory, which will be all of these things: a directory or “folder” on your computer a Git repository, linked to a remote GitHub repository an RStudio Project In the absence of other constraints, I suggest that all of your R projects have exactly this set-up. This should download the README.md file that we created on GitHub in the previous step. Look in RStudio’s file browser pane for the README.md file.&quot; 4.3.3 Template Folder Structure I keep all files associated with an analysis in a Google Drive folder, so that I can give collaborators access to raw data so they can work with it directly. Doing this has advantages, but you must be careful not to edit files directly in Google Drive without committing changes using Git. Doing that risks putting your repo out of sync with your local version. So only ever make changes file stored in your Google Drive via R, and commit changes to them using Git. I use a default folder structure for every analysis based on the files that are typically produced from every analysis. Using the directory that you created your new R-Studio project, which will also be your working directory, create these sub-folders within the project folder: raw data processed data raw scripts final scripts exploratory figures final figures I typically have my project folder set as my working directory and navigate down into the appropriate directory from there to save figures, processed data etc. using this syntax: write_csv(yourfilename, &quot;./processed data/yourfilename.csv&quot;) as an example. 4.3.4 Modularize your Analysis I usually create a series of scripts in any anlaysis, which helps me to modularize the different components of an analysis. There are several discrete steps you go through when conducting an analysis. Dr. Roger Peng suggests these steps are common to all analyses, and highlighted in bold are the steps that can be modularized in your analysis, by creating seperate scripts: Define the question Define the ideal data set Determine what data you can access Obtain the data Pre-process and Clean the data Exploratory data analysis Statistical prediction/modeling Interpret results Challenge results Synthesize/write-up results Create reproducible code 4.3.5 Clean and Pre-process Data This module of your analysis includes importing, tidying, and processing your data to get it into the stage you’d like to analyze it in. 4.3.5.1 Import your Data Most often you’re going to want to read in files that are .csv files. These are comma seperated value files and can be produced from excel or google sheets by saving your excel or google sheet file as a .csv file. The first module of an analysis I produce is a plain .R script that loads in my .csv data file and save it in my R environment as a tibble, a type of data frame, using the new_tbl &lt;- read_csv(&quot;filepath&quot;) format. Before you read in a file, you should load the packages that we will be required for every analysis you conduct using the library(tidyverse) function. Note that you should not use the base R function read.csv but rather use the tidyverse function read_csv. The base version will inevitably cause frustration due to incorrect variable class assignement. Once I have my dataset in my work environment, I’ll make sure that all the variable classes are what I want them to be. Luckily, when read_csv reads in your file, it prints a summary of the name and type of each variable in your dataframe. I always check: that my Date column has the class ‘date’ that categorical variables are stored as factors anything that is not a categorical variable is not stored as a factor but rather is numeric, character, or integer for example. One of the most common problems of why you get errors in R is from R assigning incorrect variable classes. Often you want your variable to be of class numeric, or character, but it is assigned as a factor. This is extremely common and can be remedied by coercing your variable class. 4.3.5.2 Tidy your Data Tidy data is best explained by a quote from Hadley Wickham’s paper in the Journal of Statistical Software: “Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” The ‘tidyverse’ model of working in R that I have previously referred to relies on this system of data preparation. The basic tenets that make a dataset tidy are: Each variable must have its own column Each observation must have its own row Each value must have its own cell Beyond that each type of observational unit should be its own table. That is to say, I wouldn’t mix weather metadata from field collection of a fish sample in the same table as the results of a laboratory analysis. Instead I would keep these two types of information, in different tables, but provide a common column in each that allows the tables to be later joined. This last part related to joining tables is an important concept that I will come back to. If you are invloved in producing the data set, I recommend setting up your data collection to follow the tidy data format, to make your life, or the life of your anlaysts, much easier! If your data is not tidy, there are a variety of functions you can use from the tidyr package that you can learn about in Chapter 9: Tidy Data with tidyr. But hopefully your data has arrived to you tidy, in which case you can move on to processing the data for your analysis. 4.3.5.3 Process your data There is usually some number of things that you will want to do to your data before you start exploring it. You may want to create new variables based on others, you may want to seclect only certain variables, or you may want to filter out certain observations. This would be to the time to get your data to where you want it before you start to analyze it. Select certain observations using select(), filter observations using filter() 4.3.6 Exercises Set up a new GitHUB repo called ’developing-anaylses-w-hakai Start a new R-Studio project Re-create the suggested template folder structure in the working directory you established your R project file in. Download this example dataset into the raw data folder of your project directory Make sure the data is tidy Process your data for how you want to analyze it and export it using the write_csv function to your processed data foler. 4.3.7 Exploratory Data Analysis This is where you start playing with the data. You will likely try many different summaries and visualizations while you seek a deeper understanding of your data. I typically create an exploraoty data analysis (EDA) script specifically for this. I use R Markdown files (.Rmd) for my EDA. This allows me to weave narrative explanations of what exploration I am doing in each seperate code chunk. Writing this narrative about why you chose to explore a certain analysis or summary, and why it did or didn’t work is key in documenting your analysis so you can come back to it, make sense of what you’ve done, and pick up where you left off. EDA allows me to try many different summaries or visualizations without having to worry about making the output perfect or pretty. This process will give you insight to your data as well as generate new questions and provoke new analyses. One excellent tool that can help you explore your data interactively is using shiny apps. There’s too much detail to focus on how to produce shiny apps here but suffice to say that creating interactivity with your data set is an excellent way to explore many different relationships between your data. Exploratory analysis can probably go on forever depending on the complexity of your data. You will probably think of 1000 different ways to look at your data, but after some time you need to refocus on your original question and think carefully about what analyses and visualisations are best to communicate. Because you’ve been storing all of your EDA in and EDA script you’ve got a record of every path you went down while exploring your data. You can then cherry pick the analyses, plots, and summaries you want to include in your final scripts and simply copy and past them into a new .Rmd script. 4.3.7.1 Exercises Create a to do list using issues in GitHub Produce summary plots and commits along with milestones Make a pull request Make a branch to produce a shiny app Make a pull request to merge shinny app Produce a shiny app 4.3.8 Commit, etc… use GitHUB flow 4.3.9 Make plots 4.4 GitHUB Best Practices 4.4.1 GitHUB Flow GitHUB Flow is a workflow that should generally be adhered to whether working individually or collaboratively. Some will argue that there’s no such thing as working individually, especially when you consider that you’re constantly collaborating with past you, and future you. The main concepts of GitHUB Flow are: Branching - This is a core concept in GitHUB and version cntrol in general – The master branch must always remain stable and working. When you create a new branch, you are creating an environment where you can try out new ideas without directly manipulating the stable ‘master’ version of your analysis. Commits - Commits represent significant changes to your branch. I think of them as mini-milestones that add up to your complete analysis. Perhaps you’re mini-milestone is to create a linear model of something. You can work locally and make save files on your computer while working on the linear model like you normally would. When you finish your linear model, this is a good time to make a commit to your branch. You must name each commit, and this should be descriptive. In this case ‘fitted linear model’ would be ideal. Pull Requests - I like to think of pull requests as an opportunity to review and discuss the sum of several commits before they are merged back into the main branch. Something that helped clarify what a pull request is for me, was thinking about the difference between a push (write) request, and a pull (read) request. In the context of a shared repository, a pull request gives everyone that the repo is shared with a change to read your request to consider the sum of your mini-milestones. Pull requests should amount to the completion of a component of your project. See here for more info. Issues - When working ‘individually’, an excellent way to create a TODO list for yourself is to create new ‘Issues’. Again this could be something routine, like ‘filter data to include only sampling events in the morning.’ You can then leave yourself, and potential collaborators, a little note about why you’re doing this which will be preserved in a timeline for future you, or collaborators, to inspect. Each issue in your repo is given a number, and future commits can be labelled ‘fixes issue #45’ and when that commit is wrapped up in a pull-request which is eventually merged to the master branch, the issue will be marked as resolved. When working collaboratively, issues can be created by whomever is reviewing your code, and the issue can be assigned to you to fix. Merge - Once a Pull Request has been sufficiently reviewed, discussed, and tested (deployed), the new code can be merged with the master branch, typically by the team owner, or maintainer. See this GitHUB Flow help article for a nice walkthrough with good diagrams. 4.4.2 Final Data Product What your final data product is going to be (summary report, figures for your manuscript, an interactive dashboard for a website), will dictate what your final scripts will be. As a baseline I recommend .Rmd as the final format because this gives you a lot of flexibility in terms of polished data products. 4.5 Reproducible research using markdown, notebooks, or other (Need significant additions, editing, and contributions beyond here) R Notebooks is a good way to collaborate with other analysts. When using R Notebooks it’s best to set defualts to: You want your YAML header to look something like this: output: html_notebook: default, keep_md: TRUE github_document: default 4.6 Collaboration with Phyton users The data analysis files you commit to your repo should include a ‘feather’ table. Feather is data frame format that is readable by both Python and R, and therefore every analysis should produce this type of dataframe to ensure reproducibility for Python users "],
["additional-references-and-resources.html", "5 Additional References and Resources", " 5 Additional References and Resources Journal of Biostatistics Guide for Reproducible Research Compiled Ideas on Managing a Statistical Analysis Project "]
]
