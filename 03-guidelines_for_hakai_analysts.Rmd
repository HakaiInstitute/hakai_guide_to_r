# Guidelines for Hakai Data Analysts

## Reproducible Research, Literate Programming, and the Open Notebook

These three concepts bring together a very modern way of conducting science. These are the benefits of using these methods:

* They save you work in the long run by being able to reproduce your own analyses after you've long forgotten the details of how they were conducted
* If you weave a narrative text into your computer code you'll be able to understand what you were thinking at a later time when you revisit it.
* You can easily collaborate 
* You can show your peers that you have nothing to hide in your analytical methods
* You can share your analyses in hopes that others will improve the quality of your analysis by offering their insight.

## Programming Style Guide
Using consisten formatting for writing your code, makes it easier for everyone to interpret. We loosely follow [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml). This is an excellent place to start. But you can make slight modifications at your discretion, for example; Googles guide recommendeds two space indents for new lines of code within the same function, but I prefer 8. See [Roger Pengs Coding Standards Video](https://www.youtube.com/watch?v=MSPKE1y3cyQ) for some excellent suggestions that I mostly adhere to.

## Example Project
In this section I will walk you through an example of a project that brings together all the different concepts we've been talking about up to this point. This project should serve as an example of how to develop analyses for the Hakai Institute.

### Set up a local directory

I keep all files associated with an analysis in a google drive folder, so that I can give collaborators access to RAW data so they can work with it directly.

I use a default folder structure for every analysis based on the files that are typically produced from every analysis. First I create a folder for the name of the project and create these sub-folders within the project folder.

The sub folders I use are: 
* raw data
* processed data 
* raw scripts
* final scripts
* exploratory figures
* final figures

I typically have my project folder set as my working directory and navigate down into the appropriate directory from there to save figures, processed data etc. using the "./processed data/yourfilename.csv" as an example. 


### Set up a repo
### Make an R studio Project
### Make an R markdown file
### Import data
### Commit, etc... use GitHUB flow
### Make plots


eg. Make R Markdown report?




## Organization of a Data Analysis Workflow
BJ TODO: Insert section on r projects template




## GitHUB Best Practices

### GitHUB Flow

GitHUB Flow is a workflow that should generally be adhered to whether working individually or collaboratively. Some will argue that there's no such thing as working individually, especially when you consider that you're constantly collaborating with past you, and future you.

The main concepts of GitHUB Flow are:

* **Branching** - This is a core concept in GitHUB and version cntrol in general -- The master branch must always remain stable and working. When you create a new branch, you are creating an environment where you can try out new ideas without directly manipulating the stable 'master' version of your analysis. 

* **Commits** - Commits represent significant changes to your branch. I think of them as mini-milestones that add up to your complete analysis. Perhaps you're mini-milestone is to create a linear model of something. You can work locally and make save files on your computer while working on the linear model like you normally would. When you finish your linear model, this is a good time to make a commit to your branch. You must name each commit, and this should be descriptive. In this case 'fitted linear model' would be ideal.

* **Pull Requests** - I like to think of pull requests as an opportunity to review and discuss the sum of several commits before they are merged back into the main branch. Something that helped clarify what a pull request is for me, was thinking about the difference between a push (write) request, and a pull (read) request. In the context of a shared repository, a pull request gives everyone that the repo is shared with a change to *read* your request to consider the sum of your mini-milestones. Pull requests should amount to the completion of a component of your project. See [here](https://help.github.com/articles/about-pull-requests/) for more info.

* **Issues** - When working 'individually', an excellent way to create a TODO list for yourself is to create new 'Issues'. Again this could be something routine, like 'filter data to include only sampling events in the morning.' You can then leave yourself, and potential collaborators, a little note about why you're doing this which will be preserved in a timeline for future you, or collaborators, to inspect. Each issue in your repo is given a number, and future commits can be labelled 'fixes issue #45' and when that commit is wrapped up in a pull-request which is eventually merged to the master branch, the issue will be marked as resolved. When working collaboratively, issues can be created by whomever is reviewing your code, and the issue can be assigned to you to fix.

* **Merge** - Once a Pull Request has been sufficiently reviewed, discussed, and tested (deployed), the new code can be merged with the master branch, typically by the team owner, or maintainer.

See this [GitHUB Flow](https://guides.github.com/introduction/flow/) help article for a nice walkthrough with good diagrams.

## Reproducible research using markdown, notebooks, or other (Need significant additions, editing, and contributions beyond here)

R Notebooks is a good way to collaborate with other analysts. When using R Notebooks it's best to set defualts to:
You want your YAML header to look something like this:
`output:
  html_notebook: default, 
  keep_md: TRUE
  github_document: default`

## Collaboration with Phyton users
The data analysis files you commit to your repo should include a 'feather' table. [Feather](https://blog.rstudio.org/2016/03/29/feather/) is data frame format that is readable by both Python and R, and therefore every analysis should produce this type of dataframe to ensure reproducibility for Python users
